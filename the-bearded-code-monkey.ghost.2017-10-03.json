{"db":[{"meta":{"exported_on":1507024791806,"version":"004"},"data":{"roles":[{"id":1,"uuid":"88ac695d-9bf1-4c88-8c05-20bbc2e0a1ba","name":"Administrator","description":"Administrators","created_at":"2015-11-11T19:20:25.269Z","created_by":1,"updated_at":"2015-11-11T19:20:25.269Z","updated_by":1},{"id":2,"uuid":"d7d2ccca-c360-48bb-aa56-af4daae0c9da","name":"Editor","description":"Editors","created_at":"2015-11-11T19:20:25.270Z","created_by":1,"updated_at":"2015-11-11T19:20:25.270Z","updated_by":1},{"id":3,"uuid":"a7924cc5-060d-4958-9626-d3865e8663ec","name":"Author","description":"Authors","created_at":"2015-11-11T19:20:25.270Z","created_by":1,"updated_at":"2015-11-11T19:20:25.270Z","updated_by":1},{"id":4,"uuid":"583ed2a7-fe01-4fae-8e9c-38e25ed7e2af","name":"Owner","description":"Blog Owner","created_at":"2015-11-11T19:20:25.272Z","created_by":1,"updated_at":"2015-11-11T19:20:25.272Z","updated_by":1}],"roles_users":[{"id":1,"role_id":4,"user_id":1}],"permissions":[{"id":1,"uuid":"db866fc7-8e6a-4ed2-90a0-07f306e1b1e0","name":"Export database","object_type":"db","action_type":"exportContent","object_id":null,"created_at":"2015-11-11T19:20:25.530Z","created_by":1,"updated_at":"2015-11-11T19:20:25.530Z","updated_by":1},{"id":2,"uuid":"b07d620b-05e8-403f-b340-4cc88d96dcbb","name":"Import database","object_type":"db","action_type":"importContent","object_id":null,"created_at":"2015-11-11T19:20:25.542Z","created_by":1,"updated_at":"2015-11-11T19:20:25.542Z","updated_by":1},{"id":3,"uuid":"78b4b3cc-3fbe-4bc1-8a79-c301ebe40267","name":"Delete all content","object_type":"db","action_type":"deleteAllContent","object_id":null,"created_at":"2015-11-11T19:20:25.549Z","created_by":1,"updated_at":"2015-11-11T19:20:25.549Z","updated_by":1},{"id":4,"uuid":"0dbd09fd-28af-46d4-8c82-922bd59d3fe2","name":"Send mail","object_type":"mail","action_type":"send","object_id":null,"created_at":"2015-11-11T19:20:25.556Z","created_by":1,"updated_at":"2015-11-11T19:20:25.556Z","updated_by":1},{"id":5,"uuid":"854db5a2-612c-4a74-a90b-9260343bf80d","name":"Browse notifications","object_type":"notification","action_type":"browse","object_id":null,"created_at":"2015-11-11T19:20:25.574Z","created_by":1,"updated_at":"2015-11-11T19:20:25.574Z","updated_by":1},{"id":6,"uuid":"c38e6010-afcb-4eea-b5e7-08cf88b3a31f","name":"Add notifications","object_type":"notification","action_type":"add","object_id":null,"created_at":"2015-11-11T19:20:25.581Z","created_by":1,"updated_at":"2015-11-11T19:20:25.581Z","updated_by":1},{"id":7,"uuid":"34a51b24-b48b-4822-9f0e-4fa1a02d4138","name":"Delete notifications","object_type":"notification","action_type":"destroy","object_id":null,"created_at":"2015-11-11T19:20:25.592Z","created_by":1,"updated_at":"2015-11-11T19:20:25.592Z","updated_by":1},{"id":8,"uuid":"b1e203e4-a4d0-49c5-8aef-3293cca044cc","name":"Browse posts","object_type":"post","action_type":"browse","object_id":null,"created_at":"2015-11-11T19:20:25.606Z","created_by":1,"updated_at":"2015-11-11T19:20:25.606Z","updated_by":1},{"id":9,"uuid":"358e9751-579a-40ff-a74d-a16ab5561051","name":"Read posts","object_type":"post","action_type":"read","object_id":null,"created_at":"2015-11-11T19:20:25.614Z","created_by":1,"updated_at":"2015-11-11T19:20:25.614Z","updated_by":1},{"id":10,"uuid":"2fcc0b76-167c-4bd9-bf2f-747ab0c9fced","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":"2015-11-11T19:20:25.618Z","created_by":1,"updated_at":"2015-11-11T19:20:25.618Z","updated_by":1},{"id":11,"uuid":"78dc5396-af72-44d4-b06d-2d6f2198a407","name":"Add posts","object_type":"post","action_type":"add","object_id":null,"created_at":"2015-11-11T19:20:25.622Z","created_by":1,"updated_at":"2015-11-11T19:20:25.622Z","updated_by":1},{"id":12,"uuid":"a7578adf-50f9-4bf4-8233-bd7466867151","name":"Delete posts","object_type":"post","action_type":"destroy","object_id":null,"created_at":"2015-11-11T19:20:25.629Z","created_by":1,"updated_at":"2015-11-11T19:20:25.629Z","updated_by":1},{"id":13,"uuid":"f424ee60-ff17-48aa-afdc-8c6d5f938d57","name":"Browse settings","object_type":"setting","action_type":"browse","object_id":null,"created_at":"2015-11-11T19:20:25.633Z","created_by":1,"updated_at":"2015-11-11T19:20:25.633Z","updated_by":1},{"id":14,"uuid":"01326085-8ae5-4633-9e4e-4f367bfff231","name":"Read settings","object_type":"setting","action_type":"read","object_id":null,"created_at":"2015-11-11T19:20:25.643Z","created_by":1,"updated_at":"2015-11-11T19:20:25.643Z","updated_by":1},{"id":15,"uuid":"9e493f79-9d75-414f-861b-c6bdce37e720","name":"Edit settings","object_type":"setting","action_type":"edit","object_id":null,"created_at":"2015-11-11T19:20:25.654Z","created_by":1,"updated_at":"2015-11-11T19:20:25.654Z","updated_by":1},{"id":16,"uuid":"0b3f8840-188f-4d9b-82f8-c0860ade1cfa","name":"Generate slugs","object_type":"slug","action_type":"generate","object_id":null,"created_at":"2015-11-11T19:20:25.667Z","created_by":1,"updated_at":"2015-11-11T19:20:25.667Z","updated_by":1},{"id":17,"uuid":"14b6bc3b-746b-47c4-a49d-069dbcae3ece","name":"Browse tags","object_type":"tag","action_type":"browse","object_id":null,"created_at":"2015-11-11T19:20:25.674Z","created_by":1,"updated_at":"2015-11-11T19:20:25.674Z","updated_by":1},{"id":18,"uuid":"b03b979f-c15d-4883-afc9-ddd4d0130222","name":"Read tags","object_type":"tag","action_type":"read","object_id":null,"created_at":"2015-11-11T19:20:25.678Z","created_by":1,"updated_at":"2015-11-11T19:20:25.678Z","updated_by":1},{"id":19,"uuid":"ea2522c4-5b40-4b9d-ab72-6df996127e1d","name":"Edit tags","object_type":"tag","action_type":"edit","object_id":null,"created_at":"2015-11-11T19:20:25.685Z","created_by":1,"updated_at":"2015-11-11T19:20:25.685Z","updated_by":1},{"id":20,"uuid":"7364afc3-dbf5-49d2-b147-58d72f1ed8b6","name":"Add tags","object_type":"tag","action_type":"add","object_id":null,"created_at":"2015-11-11T19:20:25.689Z","created_by":1,"updated_at":"2015-11-11T19:20:25.689Z","updated_by":1},{"id":21,"uuid":"b879215d-99e1-450a-90b8-d35cde0fbe08","name":"Delete tags","object_type":"tag","action_type":"destroy","object_id":null,"created_at":"2015-11-11T19:20:25.695Z","created_by":1,"updated_at":"2015-11-11T19:20:25.695Z","updated_by":1},{"id":22,"uuid":"d28130f8-2f4c-4843-9c5c-03c72aef7bdd","name":"Browse themes","object_type":"theme","action_type":"browse","object_id":null,"created_at":"2015-11-11T19:20:25.702Z","created_by":1,"updated_at":"2015-11-11T19:20:25.702Z","updated_by":1},{"id":23,"uuid":"92d0406e-972a-4118-bcb9-27ec17f449e0","name":"Edit themes","object_type":"theme","action_type":"edit","object_id":null,"created_at":"2015-11-11T19:20:25.706Z","created_by":1,"updated_at":"2015-11-11T19:20:25.706Z","updated_by":1},{"id":24,"uuid":"dced7151-d2f4-406d-8f50-64526706c69c","name":"Browse users","object_type":"user","action_type":"browse","object_id":null,"created_at":"2015-11-11T19:20:25.713Z","created_by":1,"updated_at":"2015-11-11T19:20:25.713Z","updated_by":1},{"id":25,"uuid":"c1858f35-e093-4a37-b9ce-0fefb194201f","name":"Read users","object_type":"user","action_type":"read","object_id":null,"created_at":"2015-11-11T19:20:25.721Z","created_by":1,"updated_at":"2015-11-11T19:20:25.721Z","updated_by":1},{"id":26,"uuid":"d3dc7d29-6b96-4b6c-b49e-5d816e129bde","name":"Edit users","object_type":"user","action_type":"edit","object_id":null,"created_at":"2015-11-11T19:20:25.724Z","created_by":1,"updated_at":"2015-11-11T19:20:25.724Z","updated_by":1},{"id":27,"uuid":"66644784-553c-4f5e-b44f-4ddeddd230a9","name":"Add users","object_type":"user","action_type":"add","object_id":null,"created_at":"2015-11-11T19:20:25.730Z","created_by":1,"updated_at":"2015-11-11T19:20:25.730Z","updated_by":1},{"id":28,"uuid":"8055821d-7288-4a95-b20f-f4c2462066d0","name":"Delete users","object_type":"user","action_type":"destroy","object_id":null,"created_at":"2015-11-11T19:20:25.746Z","created_by":1,"updated_at":"2015-11-11T19:20:25.746Z","updated_by":1},{"id":29,"uuid":"4865e1e0-d600-432e-9c39-664d43d9ea5e","name":"Assign a role","object_type":"role","action_type":"assign","object_id":null,"created_at":"2015-11-11T19:20:25.749Z","created_by":1,"updated_at":"2015-11-11T19:20:25.749Z","updated_by":1},{"id":30,"uuid":"01995635-c383-4b77-a475-62c47294e157","name":"Browse roles","object_type":"role","action_type":"browse","object_id":null,"created_at":"2015-11-11T19:20:25.759Z","created_by":1,"updated_at":"2015-11-11T19:20:25.759Z","updated_by":1}],"permissions_users":[],"permissions_roles":[{"id":1,"role_id":1,"permission_id":1},{"id":2,"role_id":1,"permission_id":2},{"id":3,"role_id":1,"permission_id":3},{"id":4,"role_id":1,"permission_id":4},{"id":5,"role_id":1,"permission_id":5},{"id":6,"role_id":1,"permission_id":6},{"id":7,"role_id":1,"permission_id":7},{"id":8,"role_id":1,"permission_id":8},{"id":9,"role_id":1,"permission_id":9},{"id":10,"role_id":1,"permission_id":10},{"id":11,"role_id":1,"permission_id":11},{"id":12,"role_id":1,"permission_id":12},{"id":13,"role_id":1,"permission_id":13},{"id":14,"role_id":1,"permission_id":14},{"id":15,"role_id":1,"permission_id":15},{"id":16,"role_id":1,"permission_id":16},{"id":17,"role_id":1,"permission_id":17},{"id":18,"role_id":1,"permission_id":18},{"id":19,"role_id":1,"permission_id":19},{"id":20,"role_id":1,"permission_id":20},{"id":21,"role_id":1,"permission_id":21},{"id":22,"role_id":1,"permission_id":22},{"id":23,"role_id":1,"permission_id":23},{"id":24,"role_id":1,"permission_id":24},{"id":25,"role_id":1,"permission_id":25},{"id":26,"role_id":1,"permission_id":26},{"id":27,"role_id":1,"permission_id":27},{"id":28,"role_id":1,"permission_id":28},{"id":29,"role_id":1,"permission_id":29},{"id":30,"role_id":1,"permission_id":30},{"id":31,"role_id":2,"permission_id":8},{"id":32,"role_id":2,"permission_id":9},{"id":33,"role_id":2,"permission_id":10},{"id":34,"role_id":2,"permission_id":11},{"id":35,"role_id":2,"permission_id":12},{"id":36,"role_id":2,"permission_id":13},{"id":37,"role_id":2,"permission_id":14},{"id":38,"role_id":2,"permission_id":16},{"id":39,"role_id":2,"permission_id":17},{"id":40,"role_id":2,"permission_id":18},{"id":41,"role_id":2,"permission_id":19},{"id":42,"role_id":2,"permission_id":20},{"id":43,"role_id":2,"permission_id":21},{"id":44,"role_id":2,"permission_id":24},{"id":45,"role_id":2,"permission_id":25},{"id":46,"role_id":2,"permission_id":26},{"id":47,"role_id":2,"permission_id":27},{"id":48,"role_id":2,"permission_id":28},{"id":49,"role_id":2,"permission_id":29},{"id":50,"role_id":2,"permission_id":30},{"id":51,"role_id":3,"permission_id":8},{"id":52,"role_id":3,"permission_id":9},{"id":53,"role_id":3,"permission_id":11},{"id":54,"role_id":3,"permission_id":13},{"id":55,"role_id":3,"permission_id":14},{"id":56,"role_id":3,"permission_id":16},{"id":57,"role_id":3,"permission_id":17},{"id":58,"role_id":3,"permission_id":18},{"id":59,"role_id":3,"permission_id":20},{"id":60,"role_id":3,"permission_id":24},{"id":61,"role_id":3,"permission_id":25},{"id":62,"role_id":3,"permission_id":30}],"permissions_apps":[],"settings":[{"id":1,"uuid":"af7457ad-ec49-4cff-b937-48ab7a70aa28","key":"databaseVersion","value":"004","type":"core","created_at":"2015-11-11T19:20:26.699Z","created_by":1,"updated_at":"2015-11-11T19:20:26.699Z","updated_by":1},{"id":13,"uuid":"a0d94c88-e21d-40d9-8154-d041846ffa7e","key":"ghost_head","value":"<script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-4800839-2', 'auto');\n  ga('send', 'pageview');\n\n</script>","type":"blog","created_at":"2015-11-11T19:20:26.704Z","created_by":1,"updated_at":"2015-11-11T21:57:46.913Z","updated_by":1},{"id":2,"uuid":"78e10be8-df41-4e92-b32b-52d639438197","key":"dbHash","value":"90784d40-6932-4a17-8849-a62112cfed2f","type":"core","created_at":"2015-11-11T19:20:26.701Z","created_by":1,"updated_at":"2015-11-11T19:20:26.894Z","updated_by":1},{"id":5,"uuid":"243a0f36-ce9a-40e9-99ab-1822faa43b57","key":"title","value":"The Bearded Code Monkey","type":"blog","created_at":"2015-11-11T19:20:26.702Z","created_by":1,"updated_at":"2015-11-11T21:57:46.915Z","updated_by":1},{"id":6,"uuid":"56ebdc33-5643-41c2-ad1e-3e6617417d21","key":"description","value":"code and things related","type":"blog","created_at":"2015-11-11T19:20:26.702Z","created_by":1,"updated_at":"2015-11-11T21:57:46.918Z","updated_by":1},{"id":18,"uuid":"31212b90-36be-46b2-933f-07f817ba1616","key":"installedApps","value":"[]","type":"app","created_at":"2015-11-11T19:20:26.705Z","created_by":1,"updated_at":"2016-12-01T03:05:41.259Z","updated_by":1},{"id":7,"uuid":"e1b31de9-e617-4c72-b4ae-03504f350430","key":"logo","value":"","type":"blog","created_at":"2015-11-11T19:20:26.703Z","created_by":1,"updated_at":"2015-11-11T21:57:46.922Z","updated_by":1},{"id":8,"uuid":"d98b8a9f-4852-4d94-b7c6-c6c8f92a9cd7","key":"cover","value":"/content/images/2015/11/blog-cover5.jpg","type":"blog","created_at":"2015-11-11T19:20:26.703Z","created_by":1,"updated_at":"2015-11-11T21:57:46.924Z","updated_by":1},{"id":17,"uuid":"b59b08e0-e7fa-43d5-95f0-898d571ee6ba","key":"activeApps","value":"[]","type":"app","created_at":"2015-11-11T19:20:26.705Z","created_by":1,"updated_at":"2015-11-11T19:28:48.318Z","updated_by":1},{"id":14,"uuid":"c590d9eb-b52f-40ef-8d4e-db77fefc0a9b","key":"ghost_foot","value":"","type":"blog","created_at":"2015-11-11T19:20:26.704Z","created_by":1,"updated_at":"2015-11-11T21:57:46.931Z","updated_by":1},{"id":15,"uuid":"bb74fecc-695f-47b3-9f3c-ac1b6f2aa42c","key":"labs","value":"{}","type":"blog","created_at":"2015-11-11T19:20:26.705Z","created_by":1,"updated_at":"2015-11-11T21:57:46.935Z","updated_by":1},{"id":16,"uuid":"8b1d055c-0dca-4d6c-99a4-6e663b1041a1","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"/\"},{\"label\":\"About\",\"url\":\"/about\"}]","type":"blog","created_at":"2015-11-11T19:20:26.705Z","created_by":1,"updated_at":"2015-11-11T21:57:46.937Z","updated_by":1},{"id":19,"uuid":"38af6f52-d7e2-4d06-ac38-413c8d507e72","key":"isPrivate","value":"false","type":"private","created_at":"2015-11-11T19:20:26.708Z","created_by":1,"updated_at":"2015-11-11T21:57:46.939Z","updated_by":1},{"id":20,"uuid":"2502db00-d226-46c4-b2f1-7a1dc2614626","key":"password","value":"null","type":"private","created_at":"2015-11-11T19:20:26.708Z","created_by":1,"updated_at":"2015-11-11T21:57:46.941Z","updated_by":1},{"id":3,"uuid":"fd5e6991-e533-48be-82ce-23a1ccb8e4c5","key":"nextUpdateCheck","value":"1507111102","type":"core","created_at":"2015-11-11T19:20:26.701Z","created_by":1,"updated_at":"2017-10-03T09:58:21.743Z","updated_by":1},{"id":4,"uuid":"2ac09115-4491-4be7-b0dd-a98c8766f7df","key":"displayUpdateNotification","value":"0.11.11","type":"core","created_at":"2015-11-11T19:20:26.701Z","created_by":1,"updated_at":"2017-10-03T09:58:21.747Z","updated_by":1},{"id":9,"uuid":"bc97e22c-a880-46b6-801a-fd9013a72ab0","key":"defaultLang","value":"en_US","type":"blog","created_at":"2015-11-11T19:20:26.703Z","created_by":1,"updated_at":"2015-11-11T21:57:46.898Z","updated_by":1},{"id":10,"uuid":"f20d8614-8136-4204-bb3a-6b2ae6c7f1c6","key":"postsPerPage","value":"5","type":"blog","created_at":"2015-11-11T19:20:26.704Z","created_by":1,"updated_at":"2015-11-11T21:57:46.902Z","updated_by":1},{"id":11,"uuid":"d0861519-df73-4b91-a99e-d1325326819c","key":"forceI18n","value":"true","type":"blog","created_at":"2015-11-11T19:20:26.704Z","created_by":1,"updated_at":"2015-11-11T21:57:46.905Z","updated_by":1},{"id":12,"uuid":"81657201-a26b-4bfc-9a1a-fb05c7a1a1aa","key":"permalinks","value":"/:slug/","type":"blog","created_at":"2015-11-11T19:20:26.704Z","created_by":1,"updated_at":"2015-11-11T21:57:46.906Z","updated_by":1},{"id":21,"uuid":"aa9084b1-16c7-4b68-a199-c86c22fbcd21","key":"activeTheme","value":"scary","type":"theme","created_at":"2015-11-11T19:20:26.705Z","created_by":1,"updated_at":"2015-11-11T21:57:46.910Z","updated_by":1}],"tags":[{"id":1,"uuid":"b0386beb-4a59-4c86-ae03-f986b142e9de","name":"Getting Started","slug":"getting-started","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-11-11T19:20:25.264Z","created_by":1,"updated_at":"2015-11-11T19:20:25.264Z","updated_by":1},{"id":2,"uuid":"1e6bc475-a144-4490-b0ee-b5c971ad0274","name":"Visual Studio","slug":"visual-studio","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-11-11T19:28:47.947Z","created_by":1,"updated_at":"2015-11-11T19:28:47.947Z","updated_by":1},{"id":3,"uuid":"31870376-af51-4324-826c-3ccbd39cb8fd","name":"git","slug":"git","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-11-11T19:28:47.949Z","created_by":1,"updated_at":"2015-11-11T19:28:47.949Z","updated_by":1},{"id":4,"uuid":"66038ade-be27-4146-bf38-b6e5487cad75","name":"Boot Camp","slug":"boot-camp","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-11-11T19:28:47.954Z","created_by":1,"updated_at":"2015-11-11T19:28:47.954Z","updated_by":1},{"id":5,"uuid":"87c6cf7e-743d-4bd7-aa9c-f5de974a6a4e","name":"OS X","slug":"os-x","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-11-11T19:28:47.956Z","created_by":1,"updated_at":"2015-11-11T19:28:47.956Z","updated_by":1},{"id":6,"uuid":"018c5fba-e231-46a0-8eca-000d15aa602f","name":"Windows 8.1","slug":"windows-8-1","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-11-11T19:28:47.957Z","created_by":1,"updated_at":"2015-11-11T19:28:47.957Z","updated_by":1},{"id":7,"uuid":"b405c609-7c74-417c-9121-769114260517","name":"blogging","slug":"blogging","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-11-11T19:28:47.959Z","created_by":1,"updated_at":"2015-11-11T19:28:47.959Z","updated_by":1},{"id":8,"uuid":"9ab52964-9563-40f1-85fa-9e56d23f0e12","name":"ghost","slug":"ghost-tag","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-11-11T19:28:47.960Z","created_by":1,"updated_at":"2015-11-11T19:28:47.960Z","updated_by":1},{"id":9,"uuid":"bcbb27f5-0a56-4d16-a649-e078df4761ae","name":"blogg","slug":"blogg","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-11-11T19:28:47.962Z","created_by":1,"updated_at":"2015-11-11T19:28:47.962Z","updated_by":1},{"id":10,"uuid":"de6b0f6c-2a9f-40c3-872d-33c202c06101","name":"RethinkDB","slug":"rethinkdb","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2016-03-14T10:38:35.149Z","created_by":1,"updated_at":"2016-03-14T10:38:35.149Z","updated_by":1},{"id":11,"uuid":"914214a9-afa5-4ea0-b9e6-2772f13e8fec","name":"NoSql","slug":"nosql","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2016-03-14T10:38:35.176Z","created_by":1,"updated_at":"2016-03-14T10:38:35.176Z","updated_by":1},{"id":12,"uuid":"f140d93b-9a5f-4fac-b256-1a72d5006856","name":"Azure","slug":"azure","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2016-04-06T00:10:21.188Z","created_by":1,"updated_at":"2016-04-06T00:10:21.188Z","updated_by":1},{"id":13,"uuid":"5fbc9dd4-6f73-4472-af4e-d53be5f6e54f","name":"Azure AD","slug":"azure-ad","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2016-04-06T00:10:21.196Z","created_by":1,"updated_at":"2016-04-06T00:10:21.196Z","updated_by":1},{"id":14,"uuid":"355c4fab-e888-4a5c-a063-5f229b0faaa2","name":"Azure AD B2C","slug":"azure-ad-b2c","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2016-04-06T00:10:21.203Z","created_by":1,"updated_at":"2016-04-06T00:10:21.203Z","updated_by":1},{"id":15,"uuid":"872a761c-7bd6-477a-8ba4-a1dbb17992a0","name":"OpenIdConnect","slug":"openidconnect","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2016-04-06T00:10:21.208Z","created_by":1,"updated_at":"2016-04-06T00:10:21.208Z","updated_by":1},{"id":16,"uuid":"6a56cc83-3afa-491f-af4e-d0925f3cadc7","name":"Authentication","slug":"authentication","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2016-04-06T00:10:21.213Z","created_by":1,"updated_at":"2016-04-06T00:10:21.213Z","updated_by":1},{"id":17,"uuid":"35eb0898-534c-4fe1-9f1e-2a415f8ea0fc","name":"Realtime","slug":"realtime","description":null,"image":null,"hidden":false,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2016-05-20T03:26:32.129Z","created_by":1,"updated_at":"2016-05-20T03:26:32.129Z","updated_by":1}],"apps":[],"app_fields":[],"client_trusted_domains":[],"posts":[{"id":7,"uuid":"b74fc03a-0197-41c9-93f1-7527aaf178d9","title":"Installing Windows 8.1 on my mac","slug":"setting-up-my-new-mac","markdown":"I recently got a new machine. And since i do a fair bit of iOS development (through Xamarin) I chose to get a mac. However, since I still do most of my work on Windows I would like to be able to boot it directly into Windows. No problem I thought, mac has Bootcamp. I'll just install the newest version of Windows using the Bootcamp assistant. Well everything went well until the Windows installation had to update the boot record. I popped a message box stating that \"Could not update the boot configuration\". No way to continue!\n\nAfter quite a bit og googling and reading I found that I found a working solution [here](https://discussions.apple.com/thread/5474320?start=585&tstart=0) (on page 40!). It's posted by a guy called **KNNSpeed**. For convenience i'll republish the solution here:\n\n> 1) Using an .iso of Windows 8.1 I bought from the Microsoft Online Store, I copied the .iso to my rMBP and pointed Bootcamp to it to make the USB installer. The image created is slightly over 4GB, so it's important to use an 8GB flashdrive. There have been reports of issues caused by using 4GB flashdrives, so 8GB or larger is the way to go.\n\n> 2) After letting the Boot Camp Assistant do its thing (I personally use a 50/50 split, but more importantly I checked all 3 checkboxes) and entering my password to install the helper tool (if you don't use a password you probably won't see the helper tool prompt), I rebooted and went right into the EFI Boot option in the boot manager (the hold-down-ALT-at-bootup menu).\n\n> 3) In Windows 8.1's setup, I got an error about installing over the BOOTCAMP partition. No problem, if we're installing Windows8.1 in native EFI mode, we can't use a single partition anyway since Windows needs to make an extra MSR partition. It can't do that when BOOTCAMP fills up all the empty space! Solution: delete that BOOTCAMP partition right there in Windows setup, and click \"New.\" You should be able to make an NTFS partition that will fit right into the empty space you just made, and you'll know you're on the right track because there'll be a second partition labelled MSR that gets created, too.\n\n> 4) Continue to install Windows as normal in the NTFS partition you just made (the non-MSR one). This is why I used the USB method; DVDs are so slow when doing Windows setup. Anyways, you should get the classic \"Could not update the boot configuration\" error at the end. DON'T PANIC, the next step fixes that.\n\n> 5) Click OK or Continue or whatever it says (it's been a while), and wait for your MacBook Pro to reboot, and go to the ALT menu. Now turn off your computer (or boot into OS X and shut it down, whatever--just turn it off without doing anything), turn it back on and do a PRAM/NVRAM reset (Hold Command+Option+P+R until you hear the chime play a second time), and go back to the ALT menu. Try not to miss the menu and boot into OS X, since I don't know if that'll mess anything up (it might, and you'll have to NVRAM reset again). Go right back into EFI Boot and proceed with Windows setup as normal.\n\n> 6) Windows should install with no errors, and when you reboot you now need to go right into the \"Windows\" drive in the ALT boot menu, as you're done with booting from the flashdrive. Also, it's OK if you go into OS X by accident now--just don't remove the flashdrive yet. If Windows reboots during this part of the installation, just use the ALT boot menu to go back into the Windows drive (remember: this isn't the \"Windows\" flashdrive icon--it's the Windows hard drive icon).\n\n> 7) Once Windows is done installing, it'll ask you to install the Bootcamp drivers. Go ahead and do that and reboot into Windows. Now go into \"This PC\" or \"Computer\" or whatever Microsoft renamed \"My Computer\" to and run \"Setup.exe\" from the flashdrive's WindowsSupport folder to install the Bootcamp drivers again. Apparently the setup that runs earlier misses a few things for some reason.\n","html":"<p>I recently got a new machine. And since i do a fair bit of iOS development (through Xamarin) I chose to get a mac. However, since I still do most of my work on Windows I would like to be able to boot it directly into Windows. No problem I thought, mac has Bootcamp. I'll just install the newest version of Windows using the Bootcamp assistant. Well everything went well until the Windows installation had to update the boot record. I popped a message box stating that \"Could not update the boot configuration\". No way to continue!</p>\n\n<p>After quite a bit og googling and reading I found that I found a working solution <a href=\"https://discussions.apple.com/thread/5474320?start=585&amp;tstart=0\">here</a> (on page 40!). It's posted by a guy called <strong>KNNSpeed</strong>. For convenience i'll republish the solution here:</p>\n\n<blockquote>\n  <p>1) Using an .iso of Windows 8.1 I bought from the Microsoft Online Store, I copied the .iso to my rMBP and pointed Bootcamp to it to make the USB installer. The image created is slightly over 4GB, so it's important to use an 8GB flashdrive. There have been reports of issues caused by using 4GB flashdrives, so 8GB or larger is the way to go.</p>\n  \n  <p>2) After letting the Boot Camp Assistant do its thing (I personally use a 50/50 split, but more importantly I checked all 3 checkboxes) and entering my password to install the helper tool (if you don't use a password you probably won't see the helper tool prompt), I rebooted and went right into the EFI Boot option in the boot manager (the hold-down-ALT-at-bootup menu).</p>\n  \n  <p>3) In Windows 8.1's setup, I got an error about installing over the BOOTCAMP partition. No problem, if we're installing Windows8.1 in native EFI mode, we can't use a single partition anyway since Windows needs to make an extra MSR partition. It can't do that when BOOTCAMP fills up all the empty space! Solution: delete that BOOTCAMP partition right there in Windows setup, and click \"New.\" You should be able to make an NTFS partition that will fit right into the empty space you just made, and you'll know you're on the right track because there'll be a second partition labelled MSR that gets created, too.</p>\n  \n  <p>4) Continue to install Windows as normal in the NTFS partition you just made (the non-MSR one). This is why I used the USB method; DVDs are so slow when doing Windows setup. Anyways, you should get the classic \"Could not update the boot configuration\" error at the end. DON'T PANIC, the next step fixes that.</p>\n  \n  <p>5) Click OK or Continue or whatever it says (it's been a while), and wait for your MacBook Pro to reboot, and go to the ALT menu. Now turn off your computer (or boot into OS X and shut it down, whatever--just turn it off without doing anything), turn it back on and do a PRAM/NVRAM reset (Hold Command+Option+P+R until you hear the chime play a second time), and go back to the ALT menu. Try not to miss the menu and boot into OS X, since I don't know if that'll mess anything up (it might, and you'll have to NVRAM reset again). Go right back into EFI Boot and proceed with Windows setup as normal.</p>\n  \n  <p>6) Windows should install with no errors, and when you reboot you now need to go right into the \"Windows\" drive in the ALT boot menu, as you're done with booting from the flashdrive. Also, it's OK if you go into OS X by accident now--just don't remove the flashdrive yet. If Windows reboots during this part of the installation, just use the ALT boot menu to go back into the Windows drive (remember: this isn't the \"Windows\" flashdrive icon--it's the Windows hard drive icon).</p>\n  \n  <p>7) Once Windows is done installing, it'll ask you to install the Bootcamp drivers. Go ahead and do that and reboot into Windows. Now go into \"This PC\" or \"Computer\" or whatever Microsoft renamed \"My Computer\" to and run \"Setup.exe\" from the flashdrive's WindowsSupport folder to install the Bootcamp drivers again. Apparently the setup that runs earlier misses a few things for some reason.</p>\n</blockquote>","image":null,"featured":false,"page":false,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-09-29T09:34:58.319Z","created_by":1,"updated_at":"2015-01-05T19:45:13.800Z","updated_by":1,"published_at":"2014-11-17T19:49:12.404Z","published_by":1},{"id":8,"uuid":"b64e4eee-b7dc-4948-893b-277ef5110872","title":"Hello blog","slug":"hello-blog","markdown":"This is an introduction post!\nOh well I guess one can't stat a blog without some kind of introduction. \nI'm software developer, a husband, a dad and a lot of other stuff. I really enjoy working in technology and I truly get thrilled when diving into new shiny languages, frameworks and tools. \n\nI'm starting this blog because I want to archive some my thoughts and programming experiments. Who know it might be useful later.\n\nStay tuned!","html":"<p>This is an introduction post! <br />\nOh well I guess one can't stat a blog without some kind of introduction. <br />\nI'm software developer, a husband, a dad and a lot of other stuff. I really enjoy working in technology and I truly get thrilled when diving into new shiny languages, frameworks and tools. </p>\n\n<p>I'm starting this blog because I want to archive some my thoughts and programming experiments. Who know it might be useful later.</p>\n\n<p>Stay tuned!</p>","image":null,"featured":false,"page":false,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-08-14T09:12:45.883Z","created_by":1,"updated_at":"2014-10-30T20:01:26.341Z","updated_by":1,"published_at":"2014-10-01T09:04:23.006Z","published_by":1},{"id":9,"uuid":"1d674c43-3ace-49b8-a3fb-7c36e27c4baf","title":"Setting up a difftool for git on windows","slug":"setting-up-a-difftool-for-git-on-windows","markdown":"Well i have been using [DiffMerge](https://sourcegear.com/diffmerge/) from SourceGear as the default diff tool in Visual Studio for quite a few years now. I use git more and more now but I never took the time to add a diff tool to the git configuration. But since i recently got a new dev machine I thought i might just set that up as well.\n\n### Installation\nI installed with [Chocolatey](http://chocolatey.org/) (remember to run cmd og PowerShell as admin) which i use to manage most of my dev tools. In fact i maintain a script on github with all the tools I use, so if you haven't used Chocolatey for application installation, now's your chance. \n\n`choco install diffmerge`\n\n### Configuraion\nThe following [Guide](https://sourcegear.com/diffmerge/webhelp/sec__git__windows__msysgit.html) is just copied from SourceGear and works at least for version 4.2.x\n\nFor diff:\n\n\tgit config --global diff.tool diffmerge\n\n\tgit config --global difftool.diffmerge.cmd \"C:/Program\\ Files/SourceGear/Common/DiffMerge/sgdm.exe `$LOCAL `$REMOTE\"\n\nFor merge:\n\t\n    git config --global merge.tool diffmerge\n\n\tgit config --global mergetool.diffmerge.trustExitCode true\n\n\tgit config --global mergetool.diffmerge.cmd \"C:/Program\\ Files/SourceGear/Common/DiffMerge/sgdm.exe /merge /result=`$MERGED `$LOCAL `$BASE `$REMOTE\\\"\n        \nWell of course you can also just edit the .gitconfig and add the settings directly into it. That would look something like this.\n\n\t[diff]\n        tool = diffmerge\n\t[difftool \"diffmerge\"]\n        cmd = \"C:/Program\\ Files/SourceGear/Common/DiffMerge/sgdm.exe $LOCAL $REMOTE\"\n\t[merge]\n        tool = diffmerge\n\t[mergetool \"diffmerge\"]\n        trustExitCode = true\n        cmd = \"C:/Program\\ Files/SourceGear/Common/DiffMerge/sgdm.exe /merge /result=$MERGED $LOCAL $BASE $REMOTE\"\n\n","html":"<p>Well i have been using <a href=\"https://sourcegear.com/diffmerge/\">DiffMerge</a> from SourceGear as the default diff tool in Visual Studio for quite a few years now. I use git more and more now but I never took the time to add a diff tool to the git configuration. But since i recently got a new dev machine I thought i might just set that up as well.</p>\n\n<h3 id=\"installation\">Installation</h3>\n\n<p>I installed with <a href=\"http://chocolatey.org/\">Chocolatey</a> (remember to run cmd og PowerShell as admin) which i use to manage most of my dev tools. In fact i maintain a script on github with all the tools I use, so if you haven't used Chocolatey for application installation, now's your chance. </p>\n\n<p><code>choco install diffmerge</code></p>\n\n<h3 id=\"configuraion\">Configuraion</h3>\n\n<p>The following <a href=\"https://sourcegear.com/diffmerge/webhelp/sec__git__windows__msysgit.html\">Guide</a> is just copied from SourceGear and works at least for version 4.2.x</p>\n\n<p>For diff:</p>\n\n<pre><code>git config --global diff.tool diffmerge\n\ngit config --global difftool.diffmerge.cmd \"C:/Program\\ Files/SourceGear/Common/DiffMerge/sgdm.exe `$LOCAL `$REMOTE\"\n</code></pre>\n\n<p>For merge:</p>\n\n<pre><code>git config --global merge.tool diffmerge\n\ngit config --global mergetool.diffmerge.trustExitCode true\n\ngit config --global mergetool.diffmerge.cmd \"C:/Program\\ Files/SourceGear/Common/DiffMerge/sgdm.exe /merge /result=`$MERGED `$LOCAL `$BASE `$REMOTE\\\"\n</code></pre>\n\n<p>Well of course you can also just edit the .gitconfig and add the settings directly into it. That would look something like this.</p>\n\n<pre><code>[diff]\n    tool = diffmerge\n[difftool \"diffmerge\"]\n    cmd = \"C:/Program\\ Files/SourceGear/Common/DiffMerge/sgdm.exe $LOCAL $REMOTE\"\n[merge]\n    tool = diffmerge\n[mergetool \"diffmerge\"]\n    trustExitCode = true\n    cmd = \"C:/Program\\ Files/SourceGear/Common/DiffMerge/sgdm.exe /merge /result=$MERGED $LOCAL $BASE $REMOTE\"\n</code></pre>","image":null,"featured":false,"page":false,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-10-01T09:58:17.968Z","created_by":1,"updated_at":"2014-10-15T11:54:40.329Z","updated_by":1,"published_at":"2014-10-01T19:57:15.494Z","published_by":1},{"id":10,"uuid":"18a1fea9-60be-45ad-9a14-f12b5abe49bd","title":"Finding big files in a git repo on windows","slug":"finding-big-files-in-a-git-repo-on-windows","markdown":"I recently joined a project that had been working on a product for a few years already. The team had used TFS for source control until just a few weeks before I joined. Because of external circumstances the team had decided to migrate the complete source tree to git (still hosted on TFS though).\n\nAfter the migration to the new git repository it turned out that the repository was quite big, to more precise it was around 1,2 GB. This could of course just be ignored since TFS repositories with a few years of age can easily grow to that size - we stick anything in there don't we. Anyway with a  recent [story in mind](http://thedailywtf.com/articles/Forever-Alone) and because we had to do a lot of cloning of the repository, I decided to have a closer look at the repo to see if we had any big files checked in by mistake.\n\n## Analyzing the repository\nI found [this](http://naleid.com/blog/2012/01/17/finding-and-purging-big-files-from-git-history) article describing exactly this task, but the article described the process with bash commands and I work work mainly on Windows and all of my team mates do too. So I thought this might just be the case for me to do a little diving into more advanced PowerShell. It turned out to take a bit longer than expected. After all git still returns text not objects even on windows.\n\n### Files and Hashes\nTo begin with, I use poshgit (installed with [Chocolatey](http://chocolatey.org)) to interact with git from PowerShell, and this is what i came up with after a bit/lot of help from my good college Uffe.\n\nFirst, get all the hashes of files in the repo.\n\n    $sortedHashes = git rev-list --objects --all | sls \"^\\w+\\s\\w\" | ConvertFrom-Csv -Delimiter ' ' -head hash,path | %{$h=@{}}{$h[$_.hash]=$_.path}{$h}\n\nDo a gc.\n\n    git gc\n\nGet the path of the index file.\n\n    $indexFile = \".git\\objects\\pack\\\" + (\".git\\objects\\pack\" | gci -Filter 'pack-*.idx')\n\nList all files ever in the repo ordered by size (this does not contain the filename and path).\n\n    $bigfiles = git verify-pack -v $indexFile | %{$_  -creplace '\\s+', ','} | sls \"^\\w+,blob\\W+[0-9]+,[0-9]+,[0-9]+$\" | ConvertFrom-Csv -Header hash, type, size | %{New-Object pscustomobject -Property @{Hash=$_.hash; Size=[int]$_.size}} | sort size -Descending\n\nMatch the hashes from the list of big files with the filenames from the sorted hashes to produce a list of the biggest files ever in the repo and send to a csv file.\n\n    $bigfiles | select size,@{n='Name';e={$sortedHashes[$_.hash]}} | Export-Csv -Delimiter ';' -Path \"big-files.csv\" -NoTypeInformation\n\nIt turned out that there was a actually a big (~10 MB) zip file in the repo i quite a few versions, but no longer in the index. Next step - get rid of it!\n\n\n## Rewriting history\nGit has a very nice tool for this - filter-branch. Here it important to be very cautious. When doing these kind of operations it is very important delete working copies of the remote repo and do a fresh clone after the changes have been pushed back. Now let's get to it.\n\nFirst, checkout all remote branches locally.\n\n    git branch -a | sls -CaseSensitive remotes |  sls -CaseSensitive -NotMatch HEAD | sls -CaseSensitive -NotMatch master |%{\n    \"git branch --track $($_ -replace '^\\s*remotes/origin/','') $_\" | iex\n    }\n\nRemove the identified file and rewrite commit history.\n\n    git filter-branch --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch THE-FILE-TO-REMOVE' --tag-name-filter cat -- --all\n\nThis can potentially take quite a bit of time...\nNow since git does makes quite an effort not to loose data all the removed files are not actually yet removed from the repo, they are just not cleaned up yet. In order to fix this it is necessary to expire all reflogs and do a gc with the aggressive flag set as here\n\n    git rm -rf .git/refs/original/\n    git reflog expire --expire=now --all\n    git gc --prune=now --aggressive\n\nSuccess! My repo has shirked to about a third of the size before the cleanup.\nLast step - push back the repository to TFS.\n\n    git push --force --all\n\nA few things started to go wrong here. The push stopped at about 7% of the upload and never resumed. After a bit of googling i realized it might be because of https. Luckily we also have a http endpoint inside our company firewall, so I just changed origin to use the default TFS http endpoint at port 8080.\n\n    git remote set-url origin http://tfs.Endpoint:8080/tfs/path.to.repo\n\nThis made the upload succeed. Everything look just as they should, I could browse the code on the TFS website and i made a new clone just as a last check. This is when the second strange thing happened. When cloning the repo it was the same size as before the cleanup!\n\nThis seems quite odd and i have not yet found an answer to why this is. I know that running the expire reflog and gc commands on the newly cloned repo will shrink it to the same size as the one I force pushed, but other than that, this is where i got to. I will file a bug report with Microsoft and keep you updated.\n\n**Update:**\nI Just got an [answer from microsoft](http://connect.microsoft.com/VisualStudio/feedback/details/1019193/unable-to-clean-a-git-repo-in-tfs) and I'll paste their answer here:\n> TFS does not currently perform garbage collection on git objects. We are aware that this is an important feature to have and we are tracking it in our backlogs. Currently, if you perform a git clone, TFS attempts to optimize for processing speed and hands you all objects that were associated with that repo without filtration, under the assumption that almost all of it will be live/reachable. After a filter operation like the one you performed, it might be prudent to delete the entire git repository from your TFS server and create a new one using the locally pruned and repacked git repo. All the standard caveats when deleting a repository apply - you lose all permission information and any TFS state associated with that repository and you will have to recreate them appropriately.\n","html":"<p>I recently joined a project that had been working on a product for a few years already. The team had used TFS for source control until just a few weeks before I joined. Because of external circumstances the team had decided to migrate the complete source tree to git (still hosted on TFS though).</p>\n\n<p>After the migration to the new git repository it turned out that the repository was quite big, to more precise it was around 1,2 GB. This could of course just be ignored since TFS repositories with a few years of age can easily grow to that size - we stick anything in there don't we. Anyway with a  recent <a href=\"http://thedailywtf.com/articles/Forever-Alone\">story in mind</a> and because we had to do a lot of cloning of the repository, I decided to have a closer look at the repo to see if we had any big files checked in by mistake.</p>\n\n<h2 id=\"analyzingtherepository\">Analyzing the repository</h2>\n\n<p>I found <a href=\"http://naleid.com/blog/2012/01/17/finding-and-purging-big-files-from-git-history\">this</a> article describing exactly this task, but the article described the process with bash commands and I work work mainly on Windows and all of my team mates do too. So I thought this might just be the case for me to do a little diving into more advanced PowerShell. It turned out to take a bit longer than expected. After all git still returns text not objects even on windows.</p>\n\n<h3 id=\"filesandhashes\">Files and Hashes</h3>\n\n<p>To begin with, I use poshgit (installed with <a href=\"http://chocolatey.org\">Chocolatey</a>) to interact with git from PowerShell, and this is what i came up with after a bit/lot of help from my good college Uffe.</p>\n\n<p>First, get all the hashes of files in the repo.</p>\n\n<pre><code>$sortedHashes = git rev-list --objects --all | sls \"^\\w+\\s\\w\" | ConvertFrom-Csv -Delimiter ' ' -head hash,path | %{$h=@{}}{$h[$_.hash]=$_.path}{$h}\n</code></pre>\n\n<p>Do a gc.</p>\n\n<pre><code>git gc\n</code></pre>\n\n<p>Get the path of the index file.</p>\n\n<pre><code>$indexFile = \".git\\objects\\pack\\\" + (\".git\\objects\\pack\" | gci -Filter 'pack-*.idx')\n</code></pre>\n\n<p>List all files ever in the repo ordered by size (this does not contain the filename and path).</p>\n\n<pre><code>$bigfiles = git verify-pack -v $indexFile | %{$_  -creplace '\\s+', ','} | sls \"^\\w+,blob\\W+[0-9]+,[0-9]+,[0-9]+$\" | ConvertFrom-Csv -Header hash, type, size | %{New-Object pscustomobject -Property @{Hash=$_.hash; Size=[int]$_.size}} | sort size -Descending\n</code></pre>\n\n<p>Match the hashes from the list of big files with the filenames from the sorted hashes to produce a list of the biggest files ever in the repo and send to a csv file.</p>\n\n<pre><code>$bigfiles | select size,@{n='Name';e={$sortedHashes[$_.hash]}} | Export-Csv -Delimiter ';' -Path \"big-files.csv\" -NoTypeInformation\n</code></pre>\n\n<p>It turned out that there was a actually a big (~10 MB) zip file in the repo i quite a few versions, but no longer in the index. Next step - get rid of it!</p>\n\n<h2 id=\"rewritinghistory\">Rewriting history</h2>\n\n<p>Git has a very nice tool for this - filter-branch. Here it important to be very cautious. When doing these kind of operations it is very important delete working copies of the remote repo and do a fresh clone after the changes have been pushed back. Now let's get to it.</p>\n\n<p>First, checkout all remote branches locally.</p>\n\n<pre><code>git branch -a | sls -CaseSensitive remotes |  sls -CaseSensitive -NotMatch HEAD | sls -CaseSensitive -NotMatch master |%{\n\"git branch --track $($_ -replace '^\\s*remotes/origin/','') $_\" | iex\n}\n</code></pre>\n\n<p>Remove the identified file and rewrite commit history.</p>\n\n<pre><code>git filter-branch --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch THE-FILE-TO-REMOVE' --tag-name-filter cat -- --all\n</code></pre>\n\n<p>This can potentially take quite a bit of time... <br />\nNow since git does makes quite an effort not to loose data all the removed files are not actually yet removed from the repo, they are just not cleaned up yet. In order to fix this it is necessary to expire all reflogs and do a gc with the aggressive flag set as here</p>\n\n<pre><code>git rm -rf .git/refs/original/\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\n</code></pre>\n\n<p>Success! My repo has shirked to about a third of the size before the cleanup. <br />\nLast step - push back the repository to TFS.</p>\n\n<pre><code>git push --force --all\n</code></pre>\n\n<p>A few things started to go wrong here. The push stopped at about 7% of the upload and never resumed. After a bit of googling i realized it might be because of https. Luckily we also have a http endpoint inside our company firewall, so I just changed origin to use the default TFS http endpoint at port 8080.</p>\n\n<pre><code>git remote set-url origin http://tfs.Endpoint:8080/tfs/path.to.repo\n</code></pre>\n\n<p>This made the upload succeed. Everything look just as they should, I could browse the code on the TFS website and i made a new clone just as a last check. This is when the second strange thing happened. When cloning the repo it was the same size as before the cleanup!</p>\n\n<p>This seems quite odd and i have not yet found an answer to why this is. I know that running the expire reflog and gc commands on the newly cloned repo will shrink it to the same size as the one I force pushed, but other than that, this is where i got to. I will file a bug report with Microsoft and keep you updated.</p>\n\n<p><strong>Update:</strong>\nI Just got an <a href=\"http://connect.microsoft.com/VisualStudio/feedback/details/1019193/unable-to-clean-a-git-repo-in-tfs\">answer from microsoft</a> and I'll paste their answer here:  </p>\n\n<blockquote>\n  <p>TFS does not currently perform garbage collection on git objects. We are aware that this is an important feature to have and we are tracking it in our backlogs. Currently, if you perform a git clone, TFS attempts to optimize for processing speed and hands you all objects that were associated with that repo without filtration, under the assumption that almost all of it will be live/reachable. After a filter operation like the one you performed, it might be prudent to delete the entire git repository from your TFS server and create a new one using the locally pruned and repacked git repo. All the standard caveats when deleting a repository apply - you lose all permission information and any TFS state associated with that repository and you will have to recreate them appropriately.</p>\n</blockquote>","image":null,"featured":false,"page":false,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-10-02T19:40:11.271Z","created_by":1,"updated_at":"2014-11-04T09:28:01.020Z","updated_by":1,"published_at":"2014-10-30T19:33:31.605Z","published_by":1},{"id":11,"uuid":"66be9ac3-6538-4cef-a421-088ae248c4f2","title":"Running a Ghost blog on Heroku","slug":"notes-on-upgrading-ghost","markdown":"#Choosing a blog\nbla bla.. ghost\nits simple, it's...\nUsed [this](http://www.therightcode.net/deploy-ghost-to-heroku-for-free/) guide by [Greg Bergé](http://www.therightcode.net/).\n#Running on Heroku\nHaving installed my blog on Heroku and using Postgres for storing data the official upgrade [guide](http://support.ghost.org/how-to-upgrade/) is not completely sufficient. This is because Ghost by is configured to use SQLite. So I just had to remember to do a npm install.\n\n    npm install pg --save\n\nThat checked in and pushed to Heroku and the blog was upgraded to the latest Ghost version.\n","html":"<h1 id=\"choosingablog\">Choosing a blog</h1>\n\n<p>bla bla.. ghost <br />\nits simple, it's... <br />\nUsed <a href=\"http://www.therightcode.net/deploy-ghost-to-heroku-for-free/\">this</a> guide by <a href=\"http://www.therightcode.net/\">Greg Bergé</a>.  </p>\n\n<h1 id=\"runningonheroku\">Running on Heroku</h1>\n\n<p>Having installed my blog on Heroku and using Postgres for storing data the official upgrade <a href=\"http://support.ghost.org/how-to-upgrade/\">guide</a> is not completely sufficient. This is because Ghost by is configured to use SQLite. So I just had to remember to do a npm install.</p>\n\n<pre><code>npm install pg --save\n</code></pre>\n\n<p>That checked in and pushed to Heroku and the blog was upgraded to the latest Ghost version.</p>","image":null,"featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-09-30T19:47:56.796Z","created_by":1,"updated_at":"2015-01-06T19:03:23.296Z","updated_by":1,"published_at":null,"published_by":null},{"id":12,"uuid":"5be57e3d-4c83-4f28-bd04-dde044181364","title":"Getting started with git in Visual Studio","slug":"getting-started-with-git-in-visual-studio","markdown":"The last year or so at [Schultz](http://schultz.dk) we have started to use git for more and more projects, and I have become the goto guy for git related issues. The last few times new developers have joined my project or started a new project using I have compiled an email with useful links on git and git from Visual Studio. So thought I might just put it in a blog post instead. \n\n\n### Getting git under your skin\nFirst of all i think it is extremely important understand the paradigm of git, and for most of my fellow .net developers the difference between a centralized source control system - like TFS, and a distributed like git. I think these resource are a great place to start:\n\n* [https://www.atlassian.com/git](https://www.atlassian.com/git)\n* [http://rogerdudler.github.io/git-guide/](http://rogerdudler.github.io/git-guide/)\n\nFor a litle deeper insight, have a look at this one:\n\n* [http://think-like-a-git.net/](http://think-like-a-git.net/)\n\n### Try it out\nWith a bit of background, just go ahead and get typing! I really recommend starting at the console, and Visual Studio will install the git client for you. But since playing around with git requires changing stuff in files, it can be a lot easier to practice in a more controlled environment. These two resources are great for that:\n\n* [http://www.wei-wang.com/ExplainGitWithD3/](http://www.wei-wang.com/ExplainGitWithD3/)\n* [http://pcottle.github.io/learnGitBranching/](http://pcottle.github.io/learnGitBranching/)\n \n### Using git form Visual Studio\nSince Visual Studio 2013 the git source control provider has been part of the installation, so no extra installs necessary. \n\nFor detailed info on git in Visual Studio, MSDN has a great tutorial here: [http://msdn.microsoft.com/en-us/library/hh850437.aspx](http://msdn.microsoft.com/en-us/library/hh850437.aspx)\n\n### A few caveats\nEven though most tasks can be accomplished from Visual Studio, I think it is necessary to get familiar with other tools like [SourceTree](http://www.sourcetreeapp.com/) or [PoshGit](https://chocolatey.org/packages/poshgit), which also just might make you more productive.\nFurther there are some very useful features of git like below are not possible form Visual Studio:\n\n* rebase\n* stash\n* cherry-pick\n\n\n### Other resources\n* [Git casts](https://www.youtube.com/playlist?list=PLttwD7NyH3omQLyVtan0CFOX_UWItX_yG)\n* [SourceTree](http://www.sourcetreeapp.com/)\n* [Git tools on chocolatey](https://chocolatey.org/packages?q=git)\n* [Setting up a merge tool](http://blog.hgaard.dk/setting-up-a-difftool-for-git-on-windows/)\n\n","html":"<p>The last year or so at <a href=\"http://schultz.dk\">Schultz</a> we have started to use git for more and more projects, and I have become the goto guy for git related issues. The last few times new developers have joined my project or started a new project using I have compiled an email with useful links on git and git from Visual Studio. So thought I might just put it in a blog post instead. </p>\n\n<h3 id=\"gettinggitunderyourskin\">Getting git under your skin</h3>\n\n<p>First of all i think it is extremely important understand the paradigm of git, and for most of my fellow .net developers the difference between a centralized source control system - like TFS, and a distributed like git. I think these resource are a great place to start:</p>\n\n<ul>\n<li><a href=\"https://www.atlassian.com/git\">https://www.atlassian.com/git</a></li>\n<li><a href=\"http://rogerdudler.github.io/git-guide/\">http://rogerdudler.github.io/git-guide/</a></li>\n</ul>\n\n<p>For a litle deeper insight, have a look at this one:</p>\n\n<ul>\n<li><a href=\"http://think-like-a-git.net/\">http://think-like-a-git.net/</a></li>\n</ul>\n\n<h3 id=\"tryitout\">Try it out</h3>\n\n<p>With a bit of background, just go ahead and get typing! I really recommend starting at the console, and Visual Studio will install the git client for you. But since playing around with git requires changing stuff in files, it can be a lot easier to practice in a more controlled environment. These two resources are great for that:</p>\n\n<ul>\n<li><a href=\"http://www.wei-wang.com/ExplainGitWithD3/\">http://www.wei-wang.com/ExplainGitWithD3/</a></li>\n<li><a href=\"http://pcottle.github.io/learnGitBranching/\">http://pcottle.github.io/learnGitBranching/</a></li>\n</ul>\n\n<h3 id=\"usinggitformvisualstudio\">Using git form Visual Studio</h3>\n\n<p>Since Visual Studio 2013 the git source control provider has been part of the installation, so no extra installs necessary. </p>\n\n<p>For detailed info on git in Visual Studio, MSDN has a great tutorial here: <a href=\"http://msdn.microsoft.com/en-us/library/hh850437.aspx\">http://msdn.microsoft.com/en-us/library/hh850437.aspx</a></p>\n\n<h3 id=\"afewcaveats\">A few caveats</h3>\n\n<p>Even though most tasks can be accomplished from Visual Studio, I think it is necessary to get familiar with other tools like <a href=\"http://www.sourcetreeapp.com/\">SourceTree</a> or <a href=\"https://chocolatey.org/packages/poshgit\">PoshGit</a>, which also just might make you more productive. <br />\nFurther there are some very useful features of git like below are not possible form Visual Studio:</p>\n\n<ul>\n<li>rebase</li>\n<li>stash</li>\n<li>cherry-pick</li>\n</ul>\n\n<h3 id=\"otherresources\">Other resources</h3>\n\n<ul>\n<li><a href=\"https://www.youtube.com/playlist?list=PLttwD7NyH3omQLyVtan0CFOX_UWItX_yG\">Git casts</a></li>\n<li><a href=\"http://www.sourcetreeapp.com/\">SourceTree</a></li>\n<li><a href=\"https://chocolatey.org/packages?q=git\">Git tools on chocolatey</a></li>\n<li><a href=\"http://blog.hgaard.dk/setting-up-a-difftool-for-git-on-windows/\">Setting up a merge tool</a></li>\n</ul>","image":null,"featured":false,"page":false,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-01-05T13:24:53.541Z","created_by":1,"updated_at":"2015-01-05T21:34:03.562Z","updated_by":1,"published_at":"2015-01-05T21:30:55.130Z","published_by":1},{"id":15,"uuid":"1794605f-79e9-4f67-945c-c4eb7c01e8b2","title":"Boxstarter","slug":"boxstarter","markdown":"I recently had to reinstall my dev machine and ..\nSo i decided to give bixstarter a try...\nWorked like a charm on my win 8.1 update image so when creating a new win 10 image ...\n\n","html":"<p>I recently had to reinstall my dev machine and .. <br />\nSo i decided to give bixstarter a try... <br />\nWorked like a charm on my win 8.1 update image so when creating a new win 10 image ...</p>","image":null,"featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-07-15T08:22:20.533Z","created_by":1,"updated_at":"2015-10-06T18:48:03.952Z","updated_by":1,"published_at":null,"published_by":null},{"id":17,"uuid":"43e0959f-8db3-42cf-b86d-4a15ed8b6adb","title":"About","slug":"about","markdown":"I am a Danish full stack software developer and architect with 10y+ experience. I do web, security cross-platform mobile development and everything in-between. I am known to be a person who challenges the well-known with a positive mind, both when it comes to building solutions, improving the development process and choosing platform, language or technology. I am inspired by functional programming languages, which is reflected in my approach to programming and I strive to become a polyglot devel- oper. Privately I am a father of two boys and a girl, married to a sociologist doing UX Research. We live in Copenhagen, but are often hanging out in our Kolonihave (Wooden hut), where we relax, build new stuff out of old stuff and try to make plants grow.","html":"<p>I am a Danish full stack software developer and architect with 10y+ experience. I do web, security cross-platform mobile development and everything in-between. I am known to be a person who challenges the well-known with a positive mind, both when it comes to building solutions, improving the development process and choosing platform, language or technology. I am inspired by functional programming languages, which is reflected in my approach to programming and I strive to become a polyglot devel- oper. Privately I am a father of two boys and a girl, married to a sociologist doing UX Research. We live in Copenhagen, but are often hanging out in our Kolonihave (Wooden hut), where we relax, build new stuff out of old stuff and try to make plants grow.</p>","image":"http://res.cloudinary.com/hnlbm8xxk/image/upload/v1414962408/blog-cover5_gkiv8s.jpg","featured":false,"page":true,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-10-01T08:06:57.264Z","created_by":1,"updated_at":"2015-08-18T08:08:00.038Z","updated_by":1,"published_at":"2014-10-01T08:07:00.000Z","published_by":1},{"id":18,"uuid":"975f4690-78b5-4d50-9f6c-eef29eb07963","title":"Building a git deploy for node web sites","slug":"building-a-git-deploy-for-node-web-sites","markdown":"# Setup deployment\nSince Rob Conery has a great course on Pluralsight - [Hacking Ghost](todo) - I'll refer to that for in detail description and just state the necessary installations here.\n##Building a post-receive script\n\nRun and develop local on a mac.\nWhen moving to my Ubuntu server on DigitalOcean, a few things needed to be updated. First of all, the path... Secondly it is necessary to add a few commands to the sudoer file\n\n### Images\nSymlink","html":"<h1 id=\"setupdeployment\">Setup deployment</h1>\n\n<p>Since Rob Conery has a great course on Pluralsight - <a href=\"todo\">Hacking Ghost</a> - I'll refer to that for in detail description and just state the necessary installations here.  </p>\n\n<h2 id=\"buildingapostreceivescript\">Building a post-receive script</h2>\n\n<p>Run and develop local on a mac. <br />\nWhen moving to my Ubuntu server on DigitalOcean, a few things needed to be updated. First of all, the path... Secondly it is necessary to add a few commands to the sudoer file</p>\n\n<h3 id=\"images\">Images</h3>\n\n<p>Symlink</p>","image":null,"featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-11-10T21:06:17.086Z","created_by":1,"updated_at":"2015-11-10T21:12:00.388Z","updated_by":1,"published_at":null,"published_by":null},{"id":19,"uuid":"f24883a7-d53d-48b0-b99d-f3606c9623d3","title":"Setting up this blog","slug":"setting-up-this-blog","markdown":"# Introduction\nI have had this blog brewing for a while, but have not really had the time to finish it before - I might get back to why in a later post.  \n\nAnyway here goes: \n\nI started out hosting it on Heroku. It's a great way of hosting web applications and you can start out on their free tier which also has a lot of free add-ons like Postgres and New Relic. How ever i like to tinker and a Pluralsight course - [Mastering Your Own Domain](https://app.pluralsight.com/library/courses/master-domain/table-of-contents) by Rob Conery - inspired me to be more in control. For instructions on how to run Ghost on Heroku I used [this](http://www.therightcode.net/deploy-ghost-to-heroku-for-free/) guide by [Greg Bergé](http://www.therightcode.net/).\n\n# The basics\nThe blogging software is [Ghost](http://tryghost.org), which is based in node and I use Postgres as datastore and have an nginx server running in front of node. The whole thing runs on a Ubuntu 15.10 server on [DigitalOcean](http://digitalocean.com).\n\n# The how\nThe Pluralsight course mentioned earlier basically describes how to setup Ghost on a new server droplet on DigitalOcean. So for detailed description have a look at that.\n\n##Setting up the server\n**Create droplet**\n\nHead over to Digital, create an account if you don't already have one and then create a new virtual machine which at Digital Ocean is called a Droplet. I use the smallest instance they provide with Ubuntu 15.10. Remember to check the backup checkbox, it's $1 more a month but having a backup is nice. Last thing; add a SSH key for convenience when logging on to the machine. When the machine is ready (usually takes around 1 minute) ssh into it to get working.\n\n   \n \n**Setup a user**\n\n    adduser ghost\n    visudo\t     # And then assign super user privileges to ghost user\n    #Add ssh key to the new user (Install ssh-copy-id with homebrew first\n    ssh-copy-id <username>@<host> \n\n**Now install all the necessary software**   \n\n    \n   \n**Install nginx**\n\n    sudo apt-get install nginx -y\n    \n**Intstall Postgres and dev lib**\n\n    sudo apt-get install postgresql libpq-dev\n\n**Add ghost user to postgress**\n\n    sudo su postgres # Switch to postgress user\n    psql             # Open terminal to postgres\n    create database <ghost-db-name>;\n    create role <db-user> with password '<uder-pw>' LOGIN; #Create role and grant login\n    alter database <ghost-db-name> owner to <db-user>; #transfer ownership\n    \n\n**Install Node**\n    \n    sudo apt-get install curl # Install curl to obtain node\n    curl --silent --location https://deb.nodesource.com/setup_0.12 | sudo bash -\n    sudo apt-get install --yes nodejs npm\n    sudo ln -s /usr/bin/nodejs /usr/bin/node # Add symlink\n\n**Install Ghost**\n\n    wget https://ghost.org/zip/ghost-0.7.1.zip # Version 0.7.1 is latest at the time of writing\n    unzip ghost-0.7.1.zip -d <folde-to-run-ghost-from>\n    \n**Prepare and start Ghost**\n\n    npm install --production\n    npm start\n    \nNow this will starts ghost on the server running the default configuration, so we'll need to edit the config.js in the root of the ghost installation to user Postgres instead of SQLite and for it to know the blog root url. The configuration should be changed to something like this:\n\n      production: {\n        url: '<blog-url>',\n        database: {\n          client: 'postgres',\n          connection: {\n              host     : 'localhost',\n              user     : '<db-user>',\n              password : '<user-pw>',\n              database : '<db-name>',\n              charset  : 'utf8'\n          },\n          debug: false\n      },\n\n\nSince I would like to hand over the webserver part to nginx (and have it runnign on port 80) i need to configure it to proxy to node. There are different ways to do so and since I'm no nginx wizard I chose the one I have used before.\n\n**Add config file to nginx**\n\n    sudo touch /etc/nginx/sites-available/ghost\n    sudo vim /etc/nginx/sites-available/ghost\n    \n**Configure proxying in nginx**\n\n    server {\n        listen 80;\n        server_name your_domain.tld;\n        location / {\n            proxy_set_header   X-Real-IP $remote_addr;\n            proxy_set_header   Host      $http_host;\n            proxy_pass         http://127.0.0.1:2368;\n        }\n\t}\n\n**Symlink to sites-enabled and restart nginx**\n\n    sudo ln -s /etc/nginx/sites-available/ghost /etc/nginx/sites-enabled/ghost\n    sudo service nginx restart\n\n**Keep ghost up and running with pm2**\n\n    sudo npm install pm2 -g \t# Install\n    pm2 startup \t\t\t\t# Ensure pm2 starts the app in system startup\n    NODE_ENV=production pm2 start index.js # Startup production instance\n    \n    \n\n## That's it\nThat concludes my basic setup of this blog. Next up:\n\n* Adding comments with Discus and insights with Google Analytics\n* Setting up deployment with git\n* Running and maintaining a Ghost blog with Docker","html":"<h1 id=\"introduction\">Introduction</h1>\n\n<p>I have had this blog brewing for a while, but have not really had the time to finish it before - I might get back to why in a later post.  </p>\n\n<p>Anyway here goes: </p>\n\n<p>I started out hosting it on Heroku. It's a great way of hosting web applications and you can start out on their free tier which also has a lot of free add-ons like Postgres and New Relic. How ever i like to tinker and a Pluralsight course - <a href=\"https://app.pluralsight.com/library/courses/master-domain/table-of-contents\">Mastering Your Own Domain</a> by Rob Conery - inspired me to be more in control. For instructions on how to run Ghost on Heroku I used <a href=\"http://www.therightcode.net/deploy-ghost-to-heroku-for-free/\">this</a> guide by <a href=\"http://www.therightcode.net/\">Greg Bergé</a>.</p>\n\n<h1 id=\"thebasics\">The basics</h1>\n\n<p>The blogging software is <a href=\"http://tryghost.org\">Ghost</a>, which is based in node and I use Postgres as datastore and have an nginx server running in front of node. The whole thing runs on a Ubuntu 15.10 server on <a href=\"http://digitalocean.com\">DigitalOcean</a>.</p>\n\n<h1 id=\"thehow\">The how</h1>\n\n<p>The Pluralsight course mentioned earlier basically describes how to setup Ghost on a new server droplet on DigitalOcean. So for detailed description have a look at that.</p>\n\n<h2 id=\"settinguptheserver\">Setting up the server</h2>\n\n<p><strong>Create droplet</strong></p>\n\n<p>Head over to Digital, create an account if you don't already have one and then create a new virtual machine which at Digital Ocean is called a Droplet. I use the smallest instance they provide with Ubuntu 15.10. Remember to check the backup checkbox, it's $1 more a month but having a backup is nice. Last thing; add a SSH key for convenience when logging on to the machine. When the machine is ready (usually takes around 1 minute) ssh into it to get working.</p>\n\n<p><strong>Setup a user</strong></p>\n\n<pre><code>adduser ghost\nvisudo         # And then assign super user privileges to ghost user\n#Add ssh key to the new user (Install ssh-copy-id with homebrew first\nssh-copy-id &lt;username&gt;@&lt;host&gt; \n</code></pre>\n\n<p><strong>Now install all the necessary software</strong>   </p>\n\n<p><strong>Install nginx</strong></p>\n\n<pre><code>sudo apt-get install nginx -y\n</code></pre>\n\n<p><strong>Intstall Postgres and dev lib</strong></p>\n\n<pre><code>sudo apt-get install postgresql libpq-dev\n</code></pre>\n\n<p><strong>Add ghost user to postgress</strong></p>\n\n<pre><code>sudo su postgres # Switch to postgress user\npsql             # Open terminal to postgres\ncreate database &lt;ghost-db-name&gt;;\ncreate role &lt;db-user&gt; with password '&lt;uder-pw&gt;' LOGIN; #Create role and grant login\nalter database &lt;ghost-db-name&gt; owner to &lt;db-user&gt;; #transfer ownership\n</code></pre>\n\n<p><strong>Install Node</strong></p>\n\n<pre><code>sudo apt-get install curl # Install curl to obtain node\ncurl --silent --location https://deb.nodesource.com/setup_0.12 | sudo bash -\nsudo apt-get install --yes nodejs npm\nsudo ln -s /usr/bin/nodejs /usr/bin/node # Add symlink\n</code></pre>\n\n<p><strong>Install Ghost</strong></p>\n\n<pre><code>wget https://ghost.org/zip/ghost-0.7.1.zip # Version 0.7.1 is latest at the time of writing\nunzip ghost-0.7.1.zip -d &lt;folde-to-run-ghost-from&gt;\n</code></pre>\n\n<p><strong>Prepare and start Ghost</strong></p>\n\n<pre><code>npm install --production\nnpm start\n</code></pre>\n\n<p>Now this will starts ghost on the server running the default configuration, so we'll need to edit the config.js in the root of the ghost installation to user Postgres instead of SQLite and for it to know the blog root url. The configuration should be changed to something like this:</p>\n\n<pre><code>  production: {\n    url: '&lt;blog-url&gt;',\n    database: {\n      client: 'postgres',\n      connection: {\n          host     : 'localhost',\n          user     : '&lt;db-user&gt;',\n          password : '&lt;user-pw&gt;',\n          database : '&lt;db-name&gt;',\n          charset  : 'utf8'\n      },\n      debug: false\n  },\n</code></pre>\n\n<p>Since I would like to hand over the webserver part to nginx (and have it runnign on port 80) i need to configure it to proxy to node. There are different ways to do so and since I'm no nginx wizard I chose the one I have used before.</p>\n\n<p><strong>Add config file to nginx</strong></p>\n\n<pre><code>sudo touch /etc/nginx/sites-available/ghost\nsudo vim /etc/nginx/sites-available/ghost\n</code></pre>\n\n<p><strong>Configure proxying in nginx</strong></p>\n\n<pre><code>server {\n    listen 80;\n    server_name your_domain.tld;\n    location / {\n        proxy_set_header   X-Real-IP $remote_addr;\n        proxy_set_header   Host      $http_host;\n        proxy_pass         http://127.0.0.1:2368;\n    }\n}\n</code></pre>\n\n<p><strong>Symlink to sites-enabled and restart nginx</strong></p>\n\n<pre><code>sudo ln -s /etc/nginx/sites-available/ghost /etc/nginx/sites-enabled/ghost\nsudo service nginx restart\n</code></pre>\n\n<p><strong>Keep ghost up and running with pm2</strong></p>\n\n<pre><code>sudo npm install pm2 -g     # Install\npm2 startup                 # Ensure pm2 starts the app in system startup\nNODE_ENV=production pm2 start index.js # Startup production instance\n</code></pre>\n\n<h2 id=\"thatsit\">That's it</h2>\n\n<p>That concludes my basic setup of this blog. Next up:</p>\n\n<ul>\n<li>Adding comments with Discus and insights with Google Analytics</li>\n<li>Setting up deployment with git</li>\n<li>Running and maintaining a Ghost blog with Docker</li>\n</ul>","image":"/content/images/2015/11/20151110_223603558_iOS-copy-1.jpg","featured":false,"page":false,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-05-26T07:58:37.347Z","created_by":1,"updated_at":"2015-11-12T21:02:18.762Z","updated_by":1,"published_at":"2015-11-12T15:48:15.315Z","published_by":1},{"id":13,"uuid":"2f90b178-b9c0-4d59-ac01-0e406ad9f118","title":"Writing a resume","slug":"creating-a-resume","markdown":"Getting it done\n===========\nOf course the meain value of a resume is the actual content of the resume, but being aprogrammer the way it's done matters :)\n\n#Latex\n- installed\nSharelatex\n\n#Jsonresume\n\n\n# Link to resume;\nhttp://fayimora.com/hosting-static-files-with-ghost/","html":"<h1 id=\"gettingitdone\">Getting it done  </h1>\n\n<p>Of course the meain value of a resume is the actual content of the resume, but being aprogrammer the way it's done matters :)</p>\n\n<h1 id=\"latex\">Latex</h1>\n\n<ul>\n<li>installed\nSharelatex</li>\n</ul>\n\n<h1 id=\"jsonresume\">Jsonresume</h1>\n\n<h1 id=\"linktoresume\">Link to resume;</h1>\n\n<p><a href=\"http://fayimora.com/hosting-static-files-with-ghost/\">http://fayimora.com/hosting-static-files-with-ghost/</a></p>","image":null,"featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-05-26T07:56:06.759Z","created_by":1,"updated_at":"2015-11-12T10:28:16.961Z","updated_by":1,"published_at":null,"published_by":null},{"id":20,"uuid":"55169199-fecf-4783-bb03-a7ae47c783db","title":"Improving application insights with elasticsearch","slug":"improving-application-insights-with-elasticsearch","markdown":"Intro, problem, thoughts and motivation. Where were we? Whys was the setup a problem?\n\n##The ELK stack\nBasics explained\n\n##Adding an elastic appender\nSink, provide whatever, setting up the server\n\n##Improving with IIS logs \nAdding Logstash (and resulting machinery)\n\n\n##Possible next steps\n* Basic performance counters\n* ?\n\n\n","html":"<p>Intro, problem, thoughts and motivation. Where were we? Whys was the setup a problem?</p>\n\n<h2 id=\"theelkstack\">The ELK stack</h2>\n\n<p>Basics explained</p>\n\n<h2 id=\"addinganelasticappender\">Adding an elastic appender</h2>\n\n<p>Sink, provide whatever, setting up the server</p>\n\n<h2 id=\"improvingwithiislogs\">Improving with IIS logs</h2>\n\n<p>Adding Logstash (and resulting machinery)</p>\n\n<h2 id=\"possiblenextsteps\">Possible next steps</h2>\n\n<ul>\n<li>Basic performance counters</li>\n<li>?</li>\n</ul>","image":null,"featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-12-02T20:36:51.508Z","created_by":1,"updated_at":"2015-12-02T20:59:32.112Z","updated_by":1,"published_at":null,"published_by":null},{"id":26,"uuid":"94fc796f-0f5e-43e0-ad0a-66ffdeafafed","title":"(Untitled)","slug":"untitled-5","markdown":"###Giving GitKrakken a go.\n\nWell I have had it installed for some months now, not really giving it a go. But about 2 weeks ago I decided to give it a proper testdrive. \nI like having a visual tool to provide me with some more overview than can be provided though the command line and i have been using [SourceTree](https://www.sourcetreeapp.com/) for that the last few years. It's a great and really powerfull tool that has helped me quite a bit in difficult situations. But it is rather slow, thus I tend to user command line for most operations and have SourceTree provide overview.\n\n\n![Krakken main screen](/content/images/2016/04/Screen-Shot-2016-04-27-at-9-42-17-PM.png)\n\n* I like to have a visual tool for sorting out branches etc...\n* There a re many different, gitk, gitextentions...\n* I'm a long time SourceTree user, but i have alweys thought it was a bit on the slow side\n* Other new, gitup? github desktop?\n* Updated with keyboard shortcuts today!\n\n![](/content/images/2016/04/Screen-Shot-2016-04-27-at-9-43-17-PM.png)\n\n###What i like about it\n* X-plat (based on [electron]() - i work on both mac and windows, nice to have a familia experience\n* fast\n* Simple when you get the hang of it\n* Dark theme - yeah really!\n* easy amend\n* pull rebase (screenshot)\n* Pull requests if origin supports it\n\n###Short commings\n* I really miss the ability to use custom diff tool. The build in is not bad, but things 3-way merge with Beyond Compare is priceless!\n* No support for 2fa?\n* interactive rebase (https://twitter.com/gitkraken/status/684543513166352384)\n* ?\n\n###In conslusion\n* Since I haven't had SourceTree open the last 4 weeks I'm probably stick to it and handle the shortcommings through the terminal.","html":"<h3 id=\"givinggitkrakkenago\">Giving GitKrakken a go.</h3>\n\n<p>Well I have had it installed for some months now, not really giving it a go. But about 2 weeks ago I decided to give it a proper testdrive. <br />\nI like having a visual tool to provide me with some more overview than can be provided though the command line and i have been using <a href=\"https://www.sourcetreeapp.com/\">SourceTree</a> for that the last few years. It's a great and really powerfull tool that has helped me quite a bit in difficult situations. But it is rather slow, thus I tend to user command line for most operations and have SourceTree provide overview.</p>\n\n<p><img src=\"/content/images/2016/04/Screen-Shot-2016-04-27-at-9-42-17-PM.png\" alt=\"Krakken main screen\" /></p>\n\n<ul>\n<li>I like to have a visual tool for sorting out branches etc...</li>\n<li>There a re many different, gitk, gitextentions...</li>\n<li>I'm a long time SourceTree user, but i have alweys thought it was a bit on the slow side</li>\n<li>Other new, gitup? github desktop?</li>\n<li>Updated with keyboard shortcuts today!</li>\n</ul>\n\n<p><img src=\"/content/images/2016/04/Screen-Shot-2016-04-27-at-9-43-17-PM.png\" alt=\"\" /></p>\n\n<h3 id=\"whatilikeaboutit\">What i like about it</h3>\n\n<ul>\n<li>X-plat (based on <a href=\"\">electron</a> - i work on both mac and windows, nice to have a familia experience</li>\n<li>fast</li>\n<li>Simple when you get the hang of it</li>\n<li>Dark theme - yeah really!</li>\n<li>easy amend</li>\n<li>pull rebase (screenshot)</li>\n<li>Pull requests if origin supports it</li>\n</ul>\n\n<h3 id=\"shortcommings\">Short commings</h3>\n\n<ul>\n<li>I really miss the ability to use custom diff tool. The build in is not bad, but things 3-way merge with Beyond Compare is priceless!</li>\n<li>No support for 2fa?</li>\n<li>interactive rebase (<a href=\"https://twitter.com/gitkraken/status/684543513166352384\">https://twitter.com/gitkraken/status/684543513166352384</a>)</li>\n<li>?</li>\n</ul>\n\n<h3 id=\"inconslusion\">In conslusion</h3>\n\n<ul>\n<li>Since I haven't had SourceTree open the last 4 weeks I'm probably stick to it and handle the shortcommings through the terminal.</li>\n</ul>","image":"/content/images/2016/04/logo.png","featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-04-20T11:46:28.130Z","created_by":1,"updated_at":"2016-05-10T01:52:22.397Z","updated_by":1,"published_at":null,"published_by":null},{"id":27,"uuid":"ef36a77e-c661-4e3f-b399-cdc8f858e20c","title":"(Untitled)","slug":"untitled-2","markdown":"I thought i might share a few good podcasts I've lisened to reccentlu.\n\n* Yehuda\n* Eloxir Jose Valim\n* Matz (Yukihiro Matsumoto)\n\n2 of them, accidently started in programming and turned out awesome!\nAll are about building!\n![](/content/images/2016/11/kicbox.png)","html":"<p>I thought i might share a few good podcasts I've lisened to reccentlu.</p>\n\n<ul>\n<li>Yehuda</li>\n<li>Eloxir Jose Valim</li>\n<li>Matz (Yukihiro Matsumoto)</li>\n</ul>\n\n<p>2 of them, accidently started in programming and turned out awesome! <br />\nAll are about building! <br />\n<img src=\"/content/images/2016/11/kicbox.png\" alt=\"\" /></p>","image":null,"featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-05-21T22:41:30.979Z","created_by":1,"updated_at":"2016-11-16T03:54:15.441Z","updated_by":1,"published_at":null,"published_by":null},{"id":21,"uuid":"277613ea-ed4b-47a7-b55a-02f186515ce3","title":"Doing stuff with RethinkDB","slug":"doinng-stuff-with-rethinkdb","markdown":"I recently started a new job. Actually I moved from Copenhagen to Brisbane, Australia with my whole family, just to try living and working in a different country - and of course to give our 3 kids and ourselves a great experience/ adventure. Anyway, I got a job in the Australian consultancy [Readify](http://readify.net) and one of the perks there is PD or Professional Development for up to 20 days a year. 20 days where I can work with/learn whatever I feel like (well almost. I need to justify the relevance of the topic and I have to produce something that my colleagues can benefit from too). Yeah I know pretty sweet! So the last 2 days I have had my first PD and my goal was to get familiar with [RethinkDB](https://www.rethinkdb.com/).\n\nWell the above is not completely true anymore. I started out writing this post when I actually did my PD, but it turns out it takes time writing stuff. Well, months have passed but here we go:\n\n## What is RethinkDB\n\nFrom [RethinkDB](http://rethinkdb.com):\n\n>RethinkDB is the first open-source, scalable JSON database built from the ground up for the realtime web. It inverts the traditional database architecture by exposing an exciting new access model – instead of polling for changes, the developer can tell RethinkDB to continuously push updated query results to applications in realtime. RethinkDB’s realtime push architecture dramatically reduces the time and effort necessary to build scalable realtime apps\n\nA few things to highlight here: Yeah, it is just a document DB, but with the access model kinda turned on it's head. This means that there is no longer a need to poll the database for updates in specific tables, they will just get pushed directly to you. In fact you can subscribe to chances for any query you write and these changes will be pushed directly to your app. That is, I think, very very powerful! RethinkDB also supports things like joins, subqueries, geo queries and map-reduce. On top of this, the fact that it is built as a distributed system, just makes it all the more interesting.\n\n**Update**\n\n*As Henrik Andersson has pointed out in the comments, I was incorrect about what kind of queries change feeds can be applied to. It is for instance not yet possible to apply change feeds to joins. For a complete description of change feeds have a look at their description [here](https://rethinkdb.com/docs/changefeeds/ruby/)*\n\n### Why should I care\n\nFirst of all think about any time you would push stuff, either to a mobile phone or to a browser. How do you usually handle that? A pattern I see very often these days in applications, where we want to ensure that all users are up to date, is the use of event/message aggregators or internal busses, where the code that writes to the database, also triggers a message to the event aggregator/bus. For these scenarios integrating RethinkDB would render the bus/aggregator redundant. Event subscribers would  simply subscribe to a query in the database instead. \n\nOther examples could be any collaborative applications where users look at the same data, IoT or just plain Pub/Sub scenarios.\n\n## What did I want to do\n\nOK, RethinkDB is some interesting tech and I wanted to get to know it little better. I decided that a logging dashboard would serve as a good example app. A simple website displaying log messages from other systems and updating the dashboard instantly when new messages arrive, simples!\n\n\n### How did I do it and how to get started.\n\nFirst of all, to get started with RethinkDB I needed to have it on my machine. I was on Windows for this one, so I just downloaded the latest version [here](https://www.rethinkdb.com/docs/install/windows/) and added to my tools folder (which is already in my PATH). Staring RethinkDB on windows is just running the ```rethinkdb.exe``` file. The data will by default be located in the same folder as where RethinkDB is started from. So to use a specific path for the data you would start RethinkDB with the ```-d``` switch and provide a path.\n\n#### The Driver\n\nIn order to connect to the database I will need a driver for it. RethinkDB officially supports drivers for JavaScript, Python, Ruby and Java, but there are community developed drivers for heaps of other [languages](https://www.rethinkdb.com/docs/install-drivers/) too. I'll be writing in C#, and luckily there is a community driver for that. Actually there are two with slightly different approach to solving the problem. I picked the one maintained by [bchavez](https://github.com/bchavez/RethinkDb.Driver). It seeks to follow the official Java driver and extends it with a more C#-ish syntax and a Linq layer. The Linq part I think is especially cool since it took a bit of time for me to internalize the default syntax. \n\n#### Writing queries and subscribing to feeds.\n\nTo get a feel for the database and the [ReQL](https://www.rethinkdb.com/docs/introduction-to-reql/) query language I started out writing simple queries from LinqPad (I have a few examples [here](https://github.com/hgaard/RethinkLogs/tree/master/query-samples)). RethinkDB also comes with a dashboard (running on localhost port 8080) that has a *data explorer* where you can write queries in the native ReQL language (JavaScript/JSON). The data explorer provides an intellisense-like experience, that proved really helpful for discovering the API surface.\n\n![Image of a console example query](/content/images/2016/08/rethinkdb-console-api.png)\n\n#### Writing the app\n\nIn order to focus on the dashboard of the app I used [Serilog](https://serilog.net/) and the Serilog [RethinkDB Sink](https://github.com/serilog/serilog-sinks-rethinkdb) to produce some \"real\" log entries.\n\nThe simple dashboard I wanted to build would just query a single table in the database and send that back to the browser.\n\n```csharp\nvar result = R.Db(Constants.LoggingDatabase).Table(LoggingTable)\n                .OrderBy(R.Desc(\"Timestamp\"))\n                .Limit(50)\n                .OrderBy(\"Timestamp\")\n                .RunResult<IList<LogEvent>>(_connection);\n```\n\nThis will the return latest 50 entries from the ```LoggingTable```. The generic ```RunResults``` will serialize the results back in a list of ```LogEvent```'s. I'll be using a query like that for the initial loading of the system.\n\nIn order to have updates streamed back to the app from the database I will have to subscribe to a change feed.\n\n```csharp\n    var feed = R.Db(Constants.LoggingDatabase)\n                .Table(Constants.LoggingTable)\n                .Changes()\n                .RunChanges<LogEvent>(connection);\n```\n\nThis will return a **cursor** object which in turn will return all updates for the query it represents. Think of the cursor as an IObservable collection. The generic ```RunChanges<LogEvent>``` will ensure that the results are returned as LogEvent objects. The driver also has a non generic version of the ```RunChanges()``` method which will return results as ```dynamic```. The objects that are pushed to the client is actually not only changes, but rather 2 objects, the new state and the one before that.\n\n#### Putting it all together\n\nSo for the logging dashboard, I wanted to subscribe to all changes from the logging table, and then push these to all clients (dashboard clients). For this I used SignalR. Adding this to the change query above, it looks like this:\n\n```csharp\n    public static void HandleUpdates(IConnectionManager connectionManager)\n    {\n        var hub = connectionManager.GetHubContext<LogHub>();\n        var connection = R.Connection()\n                        .Hostname(Constants.Host)\n                        .Port(Constants.Port)\n                        .Connect();\n        var feed = R.Db(Constants.LoggingDatabase)\n                    .Table(Constants.LoggingTable)\n                    .Changes()\n                    .RunChanges<LogEvent>(connection);\n\n\n        feed.Select(x => hub.Clients.All.onMessage(x.NewValue)).ToList();\n    }\n```\n\nA final detail worth mentioning is that in order to ensure a long lived feed/cursor, I run the query as a long running task separate from the dashboard itself.\n\n```csharp\n      Task.Factory.StartNew(() =>\n      {\n          LogHub.HandleUpdates(connManager);\n      }, TaskCreationOptions.LongRunning);\n```\n\nYou can find the complete sample application with (some basic) getting started instructions on [Github](https://github.com/hgaard/rethinklogs) including simple LinqPad query examples.\n\n## Notes on the development experience\n\nFirst of all I have to say that the official documentation on RethinkDB is excellent and very elaborate. It is usually very easy to find answers and even googling for something usually takes you to their own documentation. It also has a very good description of their overall architecture and their architectural decisions. A good example of this of the description of the trade-off's they have made in regards to the [CAP theorem](https://www.rethinkdb.com/docs/architecture/#cap-theorem).\n\nAnother interesting note is the [Jepsen tests](https://aphyr.com/posts/330-jepsen-rethinkdb-2-2-3-reconfiguration) on distributes systems correctness. It seems to indicate that the RethinkDB team has a good grip on the distributed part of building databases and are very responsive when it comes to solving problems.\n\n## Questions\n\nI also gave a lightning talk (slides [here](https://speakerdeck.com/hgaard/introduction-to-rethinkdb)) on this topic at out last [Back2Base](https://twitter.com/mouna1619/status/733515269109243907) event at Readify. Some of my colleagues asked a few good follow up questions that I thought I might share here too. \n\n\n**What concurrency model is used?**\n\nFrom the RethinkDB [blog](http://rethinkdb.com/blog/lock-free-vs-wait-free-concurrency/) there is and explanation of how RethinkDB implements a lock-free system. The tl;dr of the is; \n\n>Lock-free algorithms increase the overall throughput of a system by occasionally increasing the latency of a particular transaction. Most high- end database systems are based on lock-free algorithms, to varying degrees\n\nFrom a [architecture documentation](http://www.rethinkdb.com/docs/architecture/#how-does-rethinkdb-execute-queries), there is a more in depth description of concurrency control.\n\n**Would I user it production?**\n\nThe answer I gave at Back2Base was, that for something like dashboarding yes. RethinkDB seems very mature and has a fast growing community around it. For \"regular\" documentDB stuff, I can't really say. Mostly because I haven't used other documentDB/NoSql in important production scenarios i.e., I haven't felt the pains, and it is therefore difficult to compare the experience. But looking at the architectural decisions, the Jepsen tests and the community around RethinkDB, I would certainty not hesitate to give it a try. \n\n\n## Useful resources\n\n* RethinkDB [documentation](https://www.rethinkdb.com/docs)\n* RethinkDB [Driver](https://github.com/bchavez/RethinkDb.Driver) by Brian Chavez \n* Pluralsight - [RethinkDB fundamentals](https://www.pluralsight.com/courses/rethinkdb-fundamentals) by Rob Conery\n* The [Changelog episode](https://changelog.com/181/) with Slava Akhmechet\n","html":"<p>I recently started a new job. Actually I moved from Copenhagen to Brisbane, Australia with my whole family, just to try living and working in a different country - and of course to give our 3 kids and ourselves a great experience/ adventure. Anyway, I got a job in the Australian consultancy <a href=\"http://readify.net\">Readify</a> and one of the perks there is PD or Professional Development for up to 20 days a year. 20 days where I can work with/learn whatever I feel like (well almost. I need to justify the relevance of the topic and I have to produce something that my colleagues can benefit from too). Yeah I know pretty sweet! So the last 2 days I have had my first PD and my goal was to get familiar with <a href=\"https://www.rethinkdb.com/\">RethinkDB</a>.</p>\n\n<p>Well the above is not completely true anymore. I started out writing this post when I actually did my PD, but it turns out it takes time writing stuff. Well, months have passed but here we go:</p>\n\n<h2 id=\"whatisrethinkdb\">What is RethinkDB</h2>\n\n<p>From <a href=\"http://rethinkdb.com\">RethinkDB</a>:</p>\n\n<blockquote>\n  <p>RethinkDB is the first open-source, scalable JSON database built from the ground up for the realtime web. It inverts the traditional database architecture by exposing an exciting new access model – instead of polling for changes, the developer can tell RethinkDB to continuously push updated query results to applications in realtime. RethinkDB’s realtime push architecture dramatically reduces the time and effort necessary to build scalable realtime apps</p>\n</blockquote>\n\n<p>A few things to highlight here: Yeah, it is just a document DB, but with the access model kinda turned on it's head. This means that there is no longer a need to poll the database for updates in specific tables, they will just get pushed directly to you. In fact you can subscribe to chances for any query you write and these changes will be pushed directly to your app. That is, I think, very very powerful! RethinkDB also supports things like joins, subqueries, geo queries and map-reduce. On top of this, the fact that it is built as a distributed system, just makes it all the more interesting.</p>\n\n<p><strong>Update</strong></p>\n\n<p><em>As Henrik Andersson has pointed out in the comments, I was incorrect about what kind of queries change feeds can be applied to. It is for instance not yet possible to apply change feeds to joins. For a complete description of change feeds have a look at their description <a href=\"https://rethinkdb.com/docs/changefeeds/ruby/\">here</a></em></p>\n\n<h3 id=\"whyshouldicare\">Why should I care</h3>\n\n<p>First of all think about any time you would push stuff, either to a mobile phone or to a browser. How do you usually handle that? A pattern I see very often these days in applications, where we want to ensure that all users are up to date, is the use of event/message aggregators or internal busses, where the code that writes to the database, also triggers a message to the event aggregator/bus. For these scenarios integrating RethinkDB would render the bus/aggregator redundant. Event subscribers would  simply subscribe to a query in the database instead. </p>\n\n<p>Other examples could be any collaborative applications where users look at the same data, IoT or just plain Pub/Sub scenarios.</p>\n\n<h2 id=\"whatdidiwanttodo\">What did I want to do</h2>\n\n<p>OK, RethinkDB is some interesting tech and I wanted to get to know it little better. I decided that a logging dashboard would serve as a good example app. A simple website displaying log messages from other systems and updating the dashboard instantly when new messages arrive, simples!</p>\n\n<h3 id=\"howdididoitandhowtogetstarted\">How did I do it and how to get started.</h3>\n\n<p>First of all, to get started with RethinkDB I needed to have it on my machine. I was on Windows for this one, so I just downloaded the latest version <a href=\"https://www.rethinkdb.com/docs/install/windows/\">here</a> and added to my tools folder (which is already in my PATH). Staring RethinkDB on windows is just running the <code>rethinkdb.exe</code> file. The data will by default be located in the same folder as where RethinkDB is started from. So to use a specific path for the data you would start RethinkDB with the <code>-d</code> switch and provide a path.</p>\n\n<h4 id=\"thedriver\">The Driver</h4>\n\n<p>In order to connect to the database I will need a driver for it. RethinkDB officially supports drivers for JavaScript, Python, Ruby and Java, but there are community developed drivers for heaps of other <a href=\"https://www.rethinkdb.com/docs/install-drivers/\">languages</a> too. I'll be writing in C#, and luckily there is a community driver for that. Actually there are two with slightly different approach to solving the problem. I picked the one maintained by <a href=\"https://github.com/bchavez/RethinkDb.Driver\">bchavez</a>. It seeks to follow the official Java driver and extends it with a more C#-ish syntax and a Linq layer. The Linq part I think is especially cool since it took a bit of time for me to internalize the default syntax. </p>\n\n<h4 id=\"writingqueriesandsubscribingtofeeds\">Writing queries and subscribing to feeds.</h4>\n\n<p>To get a feel for the database and the <a href=\"https://www.rethinkdb.com/docs/introduction-to-reql/\">ReQL</a> query language I started out writing simple queries from LinqPad (I have a few examples <a href=\"https://github.com/hgaard/RethinkLogs/tree/master/query-samples\">here</a>). RethinkDB also comes with a dashboard (running on localhost port 8080) that has a <em>data explorer</em> where you can write queries in the native ReQL language (JavaScript/JSON). The data explorer provides an intellisense-like experience, that proved really helpful for discovering the API surface.</p>\n\n<p><img src=\"/content/images/2016/08/rethinkdb-console-api.png\" alt=\"Image of a console example query\" /></p>\n\n<h4 id=\"writingtheapp\">Writing the app</h4>\n\n<p>In order to focus on the dashboard of the app I used <a href=\"https://serilog.net/\">Serilog</a> and the Serilog <a href=\"https://github.com/serilog/serilog-sinks-rethinkdb\">RethinkDB Sink</a> to produce some \"real\" log entries.</p>\n\n<p>The simple dashboard I wanted to build would just query a single table in the database and send that back to the browser.</p>\n\n<pre><code class=\"language-csharp\">var result = R.Db(Constants.LoggingDatabase).Table(LoggingTable)  \n                .OrderBy(R.Desc(\"Timestamp\"))\n                .Limit(50)\n                .OrderBy(\"Timestamp\")\n                .RunResult&lt;IList&lt;LogEvent&gt;&gt;(_connection);\n</code></pre>\n\n<p>This will the return latest 50 entries from the <code>LoggingTable</code>. The generic <code>RunResults</code> will serialize the results back in a list of <code>LogEvent</code>'s. I'll be using a query like that for the initial loading of the system.</p>\n\n<p>In order to have updates streamed back to the app from the database I will have to subscribe to a change feed.</p>\n\n<pre><code class=\"language-csharp\">    var feed = R.Db(Constants.LoggingDatabase)\n                .Table(Constants.LoggingTable)\n                .Changes()\n                .RunChanges&lt;LogEvent&gt;(connection);\n</code></pre>\n\n<p>This will return a <strong>cursor</strong> object which in turn will return all updates for the query it represents. Think of the cursor as an IObservable collection. The generic <code>RunChanges&lt;LogEvent&gt;</code> will ensure that the results are returned as LogEvent objects. The driver also has a non generic version of the <code>RunChanges()</code> method which will return results as <code>dynamic</code>. The objects that are pushed to the client is actually not only changes, but rather 2 objects, the new state and the one before that.</p>\n\n<h4 id=\"puttingitalltogether\">Putting it all together</h4>\n\n<p>So for the logging dashboard, I wanted to subscribe to all changes from the logging table, and then push these to all clients (dashboard clients). For this I used SignalR. Adding this to the change query above, it looks like this:</p>\n\n<pre><code class=\"language-csharp\">    public static void HandleUpdates(IConnectionManager connectionManager)\n    {\n        var hub = connectionManager.GetHubContext&lt;LogHub&gt;();\n        var connection = R.Connection()\n                        .Hostname(Constants.Host)\n                        .Port(Constants.Port)\n                        .Connect();\n        var feed = R.Db(Constants.LoggingDatabase)\n                    .Table(Constants.LoggingTable)\n                    .Changes()\n                    .RunChanges&lt;LogEvent&gt;(connection);\n\n\n        feed.Select(x =&gt; hub.Clients.All.onMessage(x.NewValue)).ToList();\n    }\n</code></pre>\n\n<p>A final detail worth mentioning is that in order to ensure a long lived feed/cursor, I run the query as a long running task separate from the dashboard itself.</p>\n\n<pre><code class=\"language-csharp\">      Task.Factory.StartNew(() =&gt;\n      {\n          LogHub.HandleUpdates(connManager);\n      }, TaskCreationOptions.LongRunning);\n</code></pre>\n\n<p>You can find the complete sample application with (some basic) getting started instructions on <a href=\"https://github.com/hgaard/rethinklogs\">Github</a> including simple LinqPad query examples.</p>\n\n<h2 id=\"notesonthedevelopmentexperience\">Notes on the development experience</h2>\n\n<p>First of all I have to say that the official documentation on RethinkDB is excellent and very elaborate. It is usually very easy to find answers and even googling for something usually takes you to their own documentation. It also has a very good description of their overall architecture and their architectural decisions. A good example of this of the description of the trade-off's they have made in regards to the <a href=\"https://www.rethinkdb.com/docs/architecture/#cap-theorem\">CAP theorem</a>.</p>\n\n<p>Another interesting note is the <a href=\"https://aphyr.com/posts/330-jepsen-rethinkdb-2-2-3-reconfiguration\">Jepsen tests</a> on distributes systems correctness. It seems to indicate that the RethinkDB team has a good grip on the distributed part of building databases and are very responsive when it comes to solving problems.</p>\n\n<h2 id=\"questions\">Questions</h2>\n\n<p>I also gave a lightning talk (slides <a href=\"https://speakerdeck.com/hgaard/introduction-to-rethinkdb\">here</a>) on this topic at out last <a href=\"https://twitter.com/mouna1619/status/733515269109243907\">Back2Base</a> event at Readify. Some of my colleagues asked a few good follow up questions that I thought I might share here too. </p>\n\n<p><strong>What concurrency model is used?</strong></p>\n\n<p>From the RethinkDB <a href=\"http://rethinkdb.com/blog/lock-free-vs-wait-free-concurrency/\">blog</a> there is and explanation of how RethinkDB implements a lock-free system. The tl;dr of the is; </p>\n\n<blockquote>\n  <p>Lock-free algorithms increase the overall throughput of a system by occasionally increasing the latency of a particular transaction. Most high- end database systems are based on lock-free algorithms, to varying degrees</p>\n</blockquote>\n\n<p>From a <a href=\"http://www.rethinkdb.com/docs/architecture/#how-does-rethinkdb-execute-queries\">architecture documentation</a>, there is a more in depth description of concurrency control.</p>\n\n<p><strong>Would I user it production?</strong></p>\n\n<p>The answer I gave at Back2Base was, that for something like dashboarding yes. RethinkDB seems very mature and has a fast growing community around it. For \"regular\" documentDB stuff, I can't really say. Mostly because I haven't used other documentDB/NoSql in important production scenarios i.e., I haven't felt the pains, and it is therefore difficult to compare the experience. But looking at the architectural decisions, the Jepsen tests and the community around RethinkDB, I would certainty not hesitate to give it a try. </p>\n\n<h2 id=\"usefulresources\">Useful resources</h2>\n\n<ul>\n<li>RethinkDB <a href=\"https://www.rethinkdb.com/docs\">documentation</a></li>\n<li>RethinkDB <a href=\"https://github.com/bchavez/RethinkDb.Driver\">Driver</a> by Brian Chavez </li>\n<li>Pluralsight - <a href=\"https://www.pluralsight.com/courses/rethinkdb-fundamentals\">RethinkDB fundamentals</a> by Rob Conery</li>\n<li>The <a href=\"https://changelog.com/181/\">Changelog episode</a> with Slava Akhmechet</li>\n</ul>","image":"/content/images/2016/03/Rethink-Logo-1.png","featured":false,"page":false,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-03-03T09:55:19.464Z","created_by":1,"updated_at":"2016-08-18T21:47:45.716Z","updated_by":1,"published_at":"2016-08-04T10:12:27.100Z","published_by":1},{"id":24,"uuid":"3b045133-8717-46eb-8258-83a5d714703e","title":"(Untitled)","slug":"untitled-3","markdown":"Registreing users in B2C for use with business apps.\n\n\n**Notes**\n\n* Header image from [sketch](https://sketch.io/sketchpad/)\n* [http://blog.kloud.com.au/2014/12/02/how-to-best-handle-azure-ad-access-tokens-on-native-mobile-apps/](http://blog.kloud.com.au/2014/12/02/how-to-best-handle-azure-ad-access-tokens-on-native-mobile-apps/)\n\n\n\n###Next up\n* Registering users for B2C (wrapping ad graph). In order to ensure that only approves users get access to the system. We have extended a user management app to also be able to manage b2c users through the Azure AD Graph. This will have to come in a later post.\n* Mobile apps (i'm off to other matters now but will return (probaby not until B2C is out of preview) to integrate a few mobile apps written in Xamarin and Xamarin Forms witth the same 2 idp's.\n\n\n###Nifty tools:\n\n* Graph explorer https://graphexplorer.cloudapp.net\n","html":"<p>Registreing users in B2C for use with business apps.</p>\n\n<p><strong>Notes</strong></p>\n\n<ul>\n<li>Header image from <a href=\"https://sketch.io/sketchpad/\">sketch</a></li>\n<li><a href=\"http://blog.kloud.com.au/2014/12/02/how-to-best-handle-azure-ad-access-tokens-on-native-mobile-apps/\">http://blog.kloud.com.au/2014/12/02/how-to-best-handle-azure-ad-access-tokens-on-native-mobile-apps/</a></li>\n</ul>\n\n<h3 id=\"nextup\">Next up</h3>\n\n<ul>\n<li>Registering users for B2C (wrapping ad graph). In order to ensure that only approves users get access to the system. We have extended a user management app to also be able to manage b2c users through the Azure AD Graph. This will have to come in a later post.</li>\n<li>Mobile apps (i'm off to other matters now but will return (probaby not until B2C is out of preview) to integrate a few mobile apps written in Xamarin and Xamarin Forms witth the same 2 idp's.</li>\n</ul>\n\n<h3 id=\"niftytools\">Nifty tools:</h3>\n\n<ul>\n<li>Graph explorer <a href=\"https://graphexplorer.cloudapp.net\">https://graphexplorer.cloudapp.net</a></li>\n</ul>","image":null,"featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-04-06T00:02:28.944Z","created_by":1,"updated_at":"2016-04-20T12:26:18.276Z","updated_by":1,"published_at":null,"published_by":null},{"id":23,"uuid":"c1286a4e-a2a9-496f-be76-7e3d6aab1f3a","title":"Managing external users with Azure AD B2C","slug":"managing-external-users-with-azure-ad-b2c","markdown":"The last few weeks I have been working with a customer to implement Azure AD B2C login for their internal systems and I thought I might share my experience with you.\n\n###What is Azure AD B2C\nFirst of all I might need to explain a few things, like what B2C is (assuming here that you know what Azure and AD are) and what kind of problem I am solving with it. First of all the name B2C simply means business to consumer and is in preview with another new variant of AD called B2B (Business to Business). It is intended to make it easier to set up authentication with multiple social identity providers like Facebook, Google or LinkedIn as easy as can be. But B2C also provides a way to sign up users without a social identity by providing a local account type - which in turn really is just a modified version of an AD account. For the local account B2C provides all the necessary plumbing like signup, password and user management, such that the consuming application never need to store any sensitive information. You can find a lot more information in the B2C documentation [here](https://azure.microsoft.com/en-us/documentation/services/active-directory-b2c/).\n\n###The setup\nVery good then, so the customer I'm working with has some internal applications that external partners also needs access to, to cooperate with them. The applications range from web apps and web api's to mobile apps. This in essence make 3 different scenarios I had to solve: \n\n* Simple login to web app\n* Call web api from web app on behalf of user\n* Login from mobile app\n\n\nIn this post i will describe how we solved the first two scenarios and I might write about the latter in another post when I get around to actually writing the code. As I mentioned, the setup required internal as well as external users granted access to the applications. B2C only provides a way to integrate identities from other (social media) identity provides and the build in users. It does not provide any way to synchronize the B2C AD with an AD through technology like Azure AD Connect. Nor does it provide a way to federate with other AD based identity providers like \"regular\" Azure AD (which by the way is called B2E - or business to employee) or internal AD through ADFS. Thus the applications I work on need to know 2 identity providers. External user resides in B2C and internal users in an B2E existing instance. For info about how to setup B2C and the concepts of policies, please visit the [B2C documentation](https://azure.microsoft.com/en-us/documentation/services/active-directory-b2c/). The getting started sections are excellent. \n\nJust to recap in a simple authentication scenario, have a look at the illustration here[^1]\n![Azure AD login scenario](/content/images/2016/04/basics_of_auth_in_aad.png)\n\n###Initial assumption\n\nI'm already using Owin middleware for authentication, so I thought that I might just configure the middleware with another OpenID Connect idp in the Owin startup configuraion. But since it was not obvious to me how to distinguish the 2 when logging in, I decided to add the internal idp as a WS-Federation idp. This turned out to be a feasible solution for login with the web applications, since I could login with identities from both idp and have the application work as expected. All good, except that the application needs to make web api calls on behald of the user. Then I remembered how bad WS-Fed is for the modern web. WS-Fed uses SAML tokens and these won't work with the bearer token authentication in the web api. Back to the drawing board, or rather, do more googling.\n\n\n###OpenId Connect all the way\n\nIt turns out the setting up Owin with multiple idp's with the same protocol is not all that dificoult, actualy it's straight forward, they just have to be given a ``AuthenticationType`` which is just s string. For simplicity I'm just using the tenant uri.\n\n```csharp\napp.UseOpenIdConnectAuthentication(new OpenIdConnectAuthenticationOptions\n{\n    // Idp identifier \n    AuthenticationType = Auth.Config.ExternalUsersTenant,\n    ClientId = Config.ExternalUsersClientId,\n    RedirectUri = Auth.Config.RedirectUri,\n    PostLogoutRedirectUri = Auth.Config.RedirectUri,\n    Notifications = new OpenIdConnectAuthenticationNotifications\n    {\n        AuthenticationFailed = OnAuthenticationFailed,\n        RedirectToIdentityProvider = OnRedirectToIdentityProvider,\n        AuthorizationCodeReceived = OnAuthorizationCodeReceived\n    },\n    Scope = \"openid offline_access\",\n   \n    TokenValidationParameters = new System.IdentityModel.Tokens.TokenValidationParameters\n    {\n        NameClaimType = \"name\",\n    }\n});\n```\n\nAnd when logging in the same ``AuthenticationType`` is used to point out the idp when calling 'Challenge'\n\n```csharp\nif (loginType == AuthType.External)\n{\n    context.GetOwinContext().Authentication.Challenge(\n        new AuthenticationProperties(\n            new Dictionary<string, string>\n            {\n                {Config.PolicyKey, Config.SignInByEmailPolicyId}\n            })\n        {\n            RedirectUri = \"/\",\n            AllowRefresh = true,\n            IsPersistent = true\n        }, Config.ExternalUsersTenant); // Identifies idp\n}\nelse if (loginType == AuthType.Internal)\n{\n    context.GetOwinContext()\n        .Authentication.Challenge(new AuthenticationProperties\n        {\n            RedirectUri = \"/\",\n            AllowRefresh = true,\n            IsPersistent = true\n        }, Config.InternalUsersTenant);\n}\n```\n\n###Experimental problems\nExcellent, now I could login with credentials from both the B2C AD and the B2E one. Yeah, well not really. It turns out that Microsoft is also rolling out an AD v2 app model and that the library for interacting with AD - the Active Directory Access Library (ADAL) in the release with that supports B2C, is hardcoded to use v2 app model. All good except that applications that are already registered in the B2E AD are in effect not visible to the version 2 of the api. They need to be re-registered in the [new application portal](https://aadguide.azurewebsites.net/integration/registerappv2/) (which is also in preview.) in the in order login through the B2C AD. \nHmmm, I could have started the registration in the new portal, but I decided I had enough places to maintain configuration for now (B2C ADd's needs to be configured in both the old azure [management portal](https://manage.windowsazure.com) and the new ([just portal portal](https://portal.azure.com)). So I decided to have namespaces come to the rescue and use both the new (experimental) ADALv4 library and the old ADALv2. It bloats the code quite a bit in certain places, but who knows ADALv4 might have support for the old api once it gets released. I'll see if I made a wise choice. But if all goes wrong, registering the app in the v2 portal is still an option.\n\n###Producing Bearer tokens\nIn order to call the web api it's necessary to provide an access token. The acquisition of the access token follows after the sign in flow and the whole protocol is illustrated in the figrure below[^1]\n\n![Acquiring an access token](/content/images/2016/04/web_app_to_web_api.png)\n\nFor this to work with both B2C and B2E I'll have to make use of both the v2 and v4 of the ADAL library again. Here though, the difference is more profound since the version 4 has made it easier for the consumer of the api to acquire the access token. Instead of keeping track of validity of id token to determine if a refresh token should be used it is all wrapped in one method ```AcquireTokenSilent()```, which will then only fail if the user needs to reenter credentials i.e. refresh token is lost or has expired.\n\nOn the web api side the Owin pipeline needs to be configured to consume OauthBearer tokens form the 2 idp's. As with configuring the OpenIdConnect parts the idp's can be distinguished by setting the ``AuthenticationType`` accordingly.\n\n```csharp\n public void ConfigureAuth(IAppBuilder app)\n        {\n            // B2E AAD\n            app.UseOAuthBearerAuthentication(new OAuthBearerAuthenticationOptions\n            {\n                AccessTokenFormat = new JwtFormat(\n                      new TokenValidationParameters\n                      {\n                           ValidAudience = Config.InternalUsersClientId\n                      }, \n                      new OpenIdConnectCachingSecurityTokenProvider(\n                           string.Format(Config.AadInstance,\n                             Config.InternalUsersTenant, \n                             string.Empty, \n                             Config.DiscoverySuffix, \n                             string.Empty))),\n                 AuthenticationType = Config.InternalUsersTenant\n            });\n\n            // B2C AAD\n            app.UseOAuthBearerAuthentication(new OAuthBearerAuthenticationOptions\n            {\n                AccessTokenFormat = new JwtFormat(\n                      new TokenValidationParameters\n                      {\n                           ValidAudience = Config.ExternalUsersClientId\n                      }, \n                      new OpenIdConnectCachingSecurityTokenProvider(\n                           String.Format(Config.AadInstance, \n                               Config.ExternalUsersTenant, \n                               \"v2.0\", Config.DiscoverySuffix, \"?p=\"\n                               Config.CommonPolicy))),\n                AuthenticationType = Config.ExternalUsersTenant\n            });\n        }\n```\nNote that the metadata string provided for the 2 idp'S differ in that the B2C one points to version 2 of the AD api and that it also needs to reference a policy. \n\n \n###A working sample app\nI have modified and combined a few of the sample apps provided by the Azure team and published it to [Github](https://github.com/hgaard/azure-b2c-multitenant-demo) \nIt has a web app and a web api and illustrates the scenario described here\nNote that there are a few classes inherited form ms examples on B2C. These are necessary because ADAL is not yet fully updated with the necessary changes for B2C.\nA small note of caution , it is extremely important to use the latest version of Microsoft.IdentityModel.Protocol.Extensions, since the older versions will produce wrong query strings when signing in with B2C.\n\n\n### In conclusion\nEven though it surely has a few rough edges, the prospects are definetly there.\nI know there are several and more mature alternatives like [Auth0](http://auth0.com) and [Identity Server](https://github.com/IdentityServer/IdentityServer3) from Thinktecture. They are both great products, but in this case the sales point for the customer was the ability to add new external users to their systems without having to think about how to manage sensitive information like user passwords, since this is managed 100% by Azure AD B2C. As mentioned earlier I have not yet had the chance to work on extending the solution to mobile apps, but it's in the pipeline, so i might update you on that then.\n\nA note for the interested. The pre-release of B2C AD does not yet support Single Page Application (SPA) like Angular apps. Luckily for me this was not necessary this time :). I hope it will be high in the feature list for the team, since most of the projects we do these days are in that realm. \n\n**Resources**\n\n* Azure AD B2C [documentation](https://azure.microsoft.com/en-us/documentation/services/active-directory-b2c/)\n* Azure quick starts on [Github](https://github.com/AzureADQuickStarts/)\n* Azure AD App v2 [registration](https://azure.microsoft.com/en-us/documentation/articles/active-directory-appmodel-v2-overview/)\n* Authentication scenarios for Azure AD [explained](https://azure.microsoft.com/en-gb/documentation/articles/active-directory-authentication-scenarios/) \n* JWT [ Debugger](https://jwt.io/#debugger-io)\n\n[^1]: Image borrowed from [\"Authentication scenarios for Azure AD\" explained](https://azure.microsoft.com/en-gb/documentation/articles/active-directory-authentication-scenarios/) ","html":"<p>The last few weeks I have been working with a customer to implement Azure AD B2C login for their internal systems and I thought I might share my experience with you.</p>\n\n<h3 id=\"whatisazureadb2c\">What is Azure AD B2C</h3>\n\n<p>First of all I might need to explain a few things, like what B2C is (assuming here that you know what Azure and AD are) and what kind of problem I am solving with it. First of all the name B2C simply means business to consumer and is in preview with another new variant of AD called B2B (Business to Business). It is intended to make it easier to set up authentication with multiple social identity providers like Facebook, Google or LinkedIn as easy as can be. But B2C also provides a way to sign up users without a social identity by providing a local account type - which in turn really is just a modified version of an AD account. For the local account B2C provides all the necessary plumbing like signup, password and user management, such that the consuming application never need to store any sensitive information. You can find a lot more information in the B2C documentation <a href=\"https://azure.microsoft.com/en-us/documentation/services/active-directory-b2c/\">here</a>.</p>\n\n<h3 id=\"thesetup\">The setup</h3>\n\n<p>Very good then, so the customer I'm working with has some internal applications that external partners also needs access to, to cooperate with them. The applications range from web apps and web api's to mobile apps. This in essence make 3 different scenarios I had to solve: </p>\n\n<ul>\n<li>Simple login to web app</li>\n<li>Call web api from web app on behalf of user</li>\n<li>Login from mobile app</li>\n</ul>\n\n<p>In this post i will describe how we solved the first two scenarios and I might write about the latter in another post when I get around to actually writing the code. As I mentioned, the setup required internal as well as external users granted access to the applications. B2C only provides a way to integrate identities from other (social media) identity provides and the build in users. It does not provide any way to synchronize the B2C AD with an AD through technology like Azure AD Connect. Nor does it provide a way to federate with other AD based identity providers like \"regular\" Azure AD (which by the way is called B2E - or business to employee) or internal AD through ADFS. Thus the applications I work on need to know 2 identity providers. External user resides in B2C and internal users in an B2E existing instance. For info about how to setup B2C and the concepts of policies, please visit the <a href=\"https://azure.microsoft.com/en-us/documentation/services/active-directory-b2c/\">B2C documentation</a>. The getting started sections are excellent. </p>\n\n<p>Just to recap in a simple authentication scenario, have a look at the illustration here<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup> <br />\n<img src=\"/content/images/2016/04/basics_of_auth_in_aad.png\" alt=\"Azure AD login scenario\" /></p>\n\n<h3 id=\"initialassumption\">Initial assumption</h3>\n\n<p>I'm already using Owin middleware for authentication, so I thought that I might just configure the middleware with another OpenID Connect idp in the Owin startup configuraion. But since it was not obvious to me how to distinguish the 2 when logging in, I decided to add the internal idp as a WS-Federation idp. This turned out to be a feasible solution for login with the web applications, since I could login with identities from both idp and have the application work as expected. All good, except that the application needs to make web api calls on behald of the user. Then I remembered how bad WS-Fed is for the modern web. WS-Fed uses SAML tokens and these won't work with the bearer token authentication in the web api. Back to the drawing board, or rather, do more googling.</p>\n\n<h3 id=\"openidconnectalltheway\">OpenId Connect all the way</h3>\n\n<p>It turns out the setting up Owin with multiple idp's with the same protocol is not all that dificoult, actualy it's straight forward, they just have to be given a <code>AuthenticationType</code> which is just s string. For simplicity I'm just using the tenant uri.</p>\n\n<pre><code class=\"language-csharp\">app.UseOpenIdConnectAuthentication(new OpenIdConnectAuthenticationOptions  \n{\n    // Idp identifier \n    AuthenticationType = Auth.Config.ExternalUsersTenant,\n    ClientId = Config.ExternalUsersClientId,\n    RedirectUri = Auth.Config.RedirectUri,\n    PostLogoutRedirectUri = Auth.Config.RedirectUri,\n    Notifications = new OpenIdConnectAuthenticationNotifications\n    {\n        AuthenticationFailed = OnAuthenticationFailed,\n        RedirectToIdentityProvider = OnRedirectToIdentityProvider,\n        AuthorizationCodeReceived = OnAuthorizationCodeReceived\n    },\n    Scope = \"openid offline_access\",\n\n    TokenValidationParameters = new System.IdentityModel.Tokens.TokenValidationParameters\n    {\n        NameClaimType = \"name\",\n    }\n});\n</code></pre>\n\n<p>And when logging in the same <code>AuthenticationType</code> is used to point out the idp when calling 'Challenge'</p>\n\n<pre><code class=\"language-csharp\">if (loginType == AuthType.External)  \n{\n    context.GetOwinContext().Authentication.Challenge(\n        new AuthenticationProperties(\n            new Dictionary&lt;string, string&gt;\n            {\n                {Config.PolicyKey, Config.SignInByEmailPolicyId}\n            })\n        {\n            RedirectUri = \"/\",\n            AllowRefresh = true,\n            IsPersistent = true\n        }, Config.ExternalUsersTenant); // Identifies idp\n}\nelse if (loginType == AuthType.Internal)  \n{\n    context.GetOwinContext()\n        .Authentication.Challenge(new AuthenticationProperties\n        {\n            RedirectUri = \"/\",\n            AllowRefresh = true,\n            IsPersistent = true\n        }, Config.InternalUsersTenant);\n}\n</code></pre>\n\n<h3 id=\"experimentalproblems\">Experimental problems</h3>\n\n<p>Excellent, now I could login with credentials from both the B2C AD and the B2E one. Yeah, well not really. It turns out that Microsoft is also rolling out an AD v2 app model and that the library for interacting with AD - the Active Directory Access Library (ADAL) in the release with that supports B2C, is hardcoded to use v2 app model. All good except that applications that are already registered in the B2E AD are in effect not visible to the version 2 of the api. They need to be re-registered in the <a href=\"https://aadguide.azurewebsites.net/integration/registerappv2/\">new application portal</a> (which is also in preview.) in the in order login through the B2C AD. <br />\nHmmm, I could have started the registration in the new portal, but I decided I had enough places to maintain configuration for now (B2C ADd's needs to be configured in both the old azure <a href=\"https://manage.windowsazure.com\">management portal</a> and the new (<a href=\"https://portal.azure.com\">just portal portal</a>). So I decided to have namespaces come to the rescue and use both the new (experimental) ADALv4 library and the old ADALv2. It bloats the code quite a bit in certain places, but who knows ADALv4 might have support for the old api once it gets released. I'll see if I made a wise choice. But if all goes wrong, registering the app in the v2 portal is still an option.</p>\n\n<h3 id=\"producingbearertokens\">Producing Bearer tokens</h3>\n\n<p>In order to call the web api it's necessary to provide an access token. The acquisition of the access token follows after the sign in flow and the whole protocol is illustrated in the figrure below<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n\n<p><img src=\"/content/images/2016/04/web_app_to_web_api.png\" alt=\"Acquiring an access token\" /></p>\n\n<p>For this to work with both B2C and B2E I'll have to make use of both the v2 and v4 of the ADAL library again. Here though, the difference is more profound since the version 4 has made it easier for the consumer of the api to acquire the access token. Instead of keeping track of validity of id token to determine if a refresh token should be used it is all wrapped in one method <code>AcquireTokenSilent()</code>, which will then only fail if the user needs to reenter credentials i.e. refresh token is lost or has expired.</p>\n\n<p>On the web api side the Owin pipeline needs to be configured to consume OauthBearer tokens form the 2 idp's. As with configuring the OpenIdConnect parts the idp's can be distinguished by setting the <code>AuthenticationType</code> accordingly.</p>\n\n<pre><code class=\"language-csharp\"> public void ConfigureAuth(IAppBuilder app)\n        {\n            // B2E AAD\n            app.UseOAuthBearerAuthentication(new OAuthBearerAuthenticationOptions\n            {\n                AccessTokenFormat = new JwtFormat(\n                      new TokenValidationParameters\n                      {\n                           ValidAudience = Config.InternalUsersClientId\n                      }, \n                      new OpenIdConnectCachingSecurityTokenProvider(\n                           string.Format(Config.AadInstance,\n                             Config.InternalUsersTenant, \n                             string.Empty, \n                             Config.DiscoverySuffix, \n                             string.Empty))),\n                 AuthenticationType = Config.InternalUsersTenant\n            });\n\n            // B2C AAD\n            app.UseOAuthBearerAuthentication(new OAuthBearerAuthenticationOptions\n            {\n                AccessTokenFormat = new JwtFormat(\n                      new TokenValidationParameters\n                      {\n                           ValidAudience = Config.ExternalUsersClientId\n                      }, \n                      new OpenIdConnectCachingSecurityTokenProvider(\n                           String.Format(Config.AadInstance, \n                               Config.ExternalUsersTenant, \n                               \"v2.0\", Config.DiscoverySuffix, \"?p=\"\n                               Config.CommonPolicy))),\n                AuthenticationType = Config.ExternalUsersTenant\n            });\n        }\n</code></pre>\n\n<p>Note that the metadata string provided for the 2 idp'S differ in that the B2C one points to version 2 of the AD api and that it also needs to reference a policy. </p>\n\n<h3 id=\"aworkingsampleapp\">A working sample app</h3>\n\n<p>I have modified and combined a few of the sample apps provided by the Azure team and published it to <a href=\"https://github.com/hgaard/azure-b2c-multitenant-demo\">Github</a> <br />\nIt has a web app and a web api and illustrates the scenario described here <br />\nNote that there are a few classes inherited form ms examples on B2C. These are necessary because ADAL is not yet fully updated with the necessary changes for B2C. <br />\nA small note of caution , it is extremely important to use the latest version of Microsoft.IdentityModel.Protocol.Extensions, since the older versions will produce wrong query strings when signing in with B2C.</p>\n\n<h3 id=\"inconclusion\">In conclusion</h3>\n\n<p>Even though it surely has a few rough edges, the prospects are definetly there. <br />\nI know there are several and more mature alternatives like <a href=\"http://auth0.com\">Auth0</a> and <a href=\"https://github.com/IdentityServer/IdentityServer3\">Identity Server</a> from Thinktecture. They are both great products, but in this case the sales point for the customer was the ability to add new external users to their systems without having to think about how to manage sensitive information like user passwords, since this is managed 100% by Azure AD B2C. As mentioned earlier I have not yet had the chance to work on extending the solution to mobile apps, but it's in the pipeline, so i might update you on that then.</p>\n\n<p>A note for the interested. The pre-release of B2C AD does not yet support Single Page Application (SPA) like Angular apps. Luckily for me this was not necessary this time :). I hope it will be high in the feature list for the team, since most of the projects we do these days are in that realm. </p>\n\n<p><strong>Resources</strong></p>\n\n<ul>\n<li>Azure AD B2C <a href=\"https://azure.microsoft.com/en-us/documentation/services/active-directory-b2c/\">documentation</a></li>\n<li>Azure quick starts on <a href=\"https://github.com/AzureADQuickStarts/\">Github</a></li>\n<li>Azure AD App v2 <a href=\"https://azure.microsoft.com/en-us/documentation/articles/active-directory-appmodel-v2-overview/\">registration</a></li>\n<li>Authentication scenarios for Azure AD <a href=\"https://azure.microsoft.com/en-gb/documentation/articles/active-directory-authentication-scenarios/\">explained</a> </li>\n<li>JWT <a href=\"https://jwt.io/#debugger-io\"> Debugger</a></li>\n</ul>\n\n<div class=\"footnotes\"><ol><li class=\"footnote\" id=\"fn:1\"><p>Image borrowed from <a href=\"https://azure.microsoft.com/en-gb/documentation/articles/active-directory-authentication-scenarios/\">\"Authentication scenarios for Azure AD\" explained</a>  <a href=\"#fnref:1\" title=\"return to article\">↩</a></p></li></ol></div>","image":"/content/images/2016/04/basics_of_auth_in_aad-1.png","featured":false,"page":false,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-03-23T03:25:10.833Z","created_by":1,"updated_at":"2016-04-21T12:01:50.750Z","updated_by":1,"published_at":"2016-04-21T12:01:00.000Z","published_by":1},{"id":22,"uuid":"abd529ff-d8b6-4d24-85d3-f80c31c440bc","title":"(Untitled)","slug":"untitled","markdown":"Tweeks to a ghost blog\n\n**Comments with Disqus**\n\nhttps://help.disqus.com/customer/portal/articles/1454924-ghost-installation-instructions\n\n**Comments with Facebook**\n\nhttps://www.allaboutghost.com/how-to-add-facebook-comments-to-ghost/\n\n**Google Analytics**\n\nhttps://www.ghostforbeginners.com/how-to-add-google-analytics-to-ghost/\n\n**Reading time**\nhttp://devangst.com/ghost-theme-add-reading-time-to-your-blog/\nhttps://www.google.com.au/search?sourceid=chrome-psyapi2&ion=1&espv=2&ie=UTF-8&q=ghost%20add%20read%20length&oq=ghost%20add%20read%20length&aqs=chrome..69i57.6002j0j7\n\n**Notes**\n\nhttps://www.digitalocean.com/community/tutorials/how-to-create-a-blog-with-ghost-and-nginx-on-ubuntu-14-04","html":"<p>Tweeks to a ghost blog</p>\n\n<p><strong>Comments with Disqus</strong></p>\n\n<p><a href=\"https://help.disqus.com/customer/portal/articles/1454924-ghost-installation-instructions\">https://help.disqus.com/customer/portal/articles/1454924-ghost-installation-instructions</a></p>\n\n<p><strong>Comments with Facebook</strong></p>\n\n<p><a href=\"https://www.allaboutghost.com/how-to-add-facebook-comments-to-ghost/\">https://www.allaboutghost.com/how-to-add-facebook-comments-to-ghost/</a></p>\n\n<p><strong>Google Analytics</strong></p>\n\n<p><a href=\"https://www.ghostforbeginners.com/how-to-add-google-analytics-to-ghost/\">https://www.ghostforbeginners.com/how-to-add-google-analytics-to-ghost/</a></p>\n\n<p><strong>Reading time</strong>\n<a href=\"http://devangst.com/ghost-theme-add-reading-time-to-your-blog/\">http://devangst.com/ghost-theme-add-reading-time-to-your-blog/</a> <br />\n<a href=\"https://www.google.com.au/search?sourceid=chrome-psyapi2&amp;ion=1&amp;espv=2&amp;ie=UTF-8&amp;q=ghost%20add%20read%20length&amp;oq=ghost%20add%20read%20length&amp;aqs=chrome..69i57.6002j0j7\">https://www.google.com.au/search?sourceid=chrome-psyapi2&amp;ion=1&amp;espv=2&amp;ie=UTF-8&amp;q=ghost%20add%20read%20length&amp;oq=ghost%20add%20read%20length&amp;aqs=chrome..69i57.6002j0j7</a></p>\n\n<p><strong>Notes</strong></p>\n\n<p><a href=\"https://www.digitalocean.com/community/tutorials/how-to-create-a-blog-with-ghost-and-nginx-on-ubuntu-14-04\">https://www.digitalocean.com/community/tutorials/how-to-create-a-blog-with-ghost-and-nginx-on-ubuntu-14-04</a></p>","image":null,"featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-03-20T23:39:47.378Z","created_by":1,"updated_at":"2016-04-06T01:14:40.838Z","updated_by":1,"published_at":null,"published_by":null},{"id":25,"uuid":"11df1cf1-8714-44ef-9a75-020f3eb34839","title":"(Untitled)","slug":"untitled-4","markdown":"Rider pre ride...","html":"<p>Rider pre ride...</p>","image":null,"featured":false,"page":false,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-04-11T10:00:03.268Z","created_by":1,"updated_at":"2016-04-27T10:41:06.207Z","updated_by":1,"published_at":null,"published_by":null}],"posts_tags":[{"id":2,"post_id":7,"tag_id":4,"sort_order":0},{"id":3,"post_id":11,"tag_id":7,"sort_order":0},{"id":4,"post_id":12,"tag_id":2,"sort_order":0},{"id":5,"post_id":19,"tag_id":1,"sort_order":0},{"id":6,"post_id":7,"tag_id":5,"sort_order":1},{"id":7,"post_id":11,"tag_id":8,"sort_order":1},{"id":8,"post_id":12,"tag_id":3,"sort_order":1},{"id":9,"post_id":19,"tag_id":8,"sort_order":1},{"id":10,"post_id":7,"tag_id":6,"sort_order":2},{"id":11,"post_id":19,"tag_id":9,"sort_order":2},{"id":15,"post_id":23,"tag_id":12,"sort_order":0},{"id":16,"post_id":23,"tag_id":13,"sort_order":1},{"id":17,"post_id":23,"tag_id":14,"sort_order":2},{"id":18,"post_id":23,"tag_id":15,"sort_order":3},{"id":19,"post_id":23,"tag_id":16,"sort_order":4},{"id":12,"post_id":21,"tag_id":10,"sort_order":0},{"id":13,"post_id":21,"tag_id":11,"sort_order":1},{"id":20,"post_id":21,"tag_id":1,"sort_order":2},{"id":21,"post_id":21,"tag_id":17,"sort_order":3}],"app_settings":[],"users":[{"id":1,"uuid":"3dac5bec-901d-4a60-8479-82359f5f9c63","name":"Jakob Højgaard","slug":"jakob","password":"$2a$10$DqEA//hr/v58RCBWMmw/CuSKa7Mq.xGhJD64GsQugoqQjkGsjGmae","email":"jakob@hgaard.dk","image":"//www.gravatar.com/avatar/b7723b4f9d476e9b142449f23ba3efb2?s=250&d=mm&r=x","cover":null,"bio":null,"website":null,"location":null,"accessibility":null,"status":"active","language":"en_US","meta_title":null,"meta_description":null,"tour":null,"last_login":"2017-10-03T09:58:46.132Z","created_at":"2015-11-11T19:20:26.648Z","created_by":1,"updated_at":"2017-10-03T09:58:46.133Z","updated_by":1}]}}]}